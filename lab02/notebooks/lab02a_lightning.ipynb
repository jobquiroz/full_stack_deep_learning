{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f008adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/full_stack_deep_learning/lab02/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff480bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jobquiroz/full_stack_deep_learning/lab02/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing path\n",
    "import os\n",
    "\n",
    "os.getcwd()   # Verify where it is right now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution, go to lab directory:\n",
    "os.chdir('/home/jobquiroz/full_stack_deep_learning/lab02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0be90b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_recognizer', 'notebooks', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26794ca",
   "metadata": {},
   "source": [
    "## PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c5c7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pytorch-lightning.readthedocs.io/en/1.6.3/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "version = pl.__version__\n",
    "\n",
    "docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/\"  # version can also be latest, stable\n",
    "docs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a659217",
   "metadata": {},
   "source": [
    "### `pl.LightningModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e180d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "issubclass(pl.LightningModule, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58e0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models import BaseLitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a74b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
    "\n",
    "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
    "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
    "\n",
    "    # optionally, define a forward method\n",
    "    def forward(self, xs):\n",
    "        return self.model(xs)  # we like to just call the model's forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78112d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "\tNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\n",
      "\t`training_step()`, `train_dataloader()` and `configure_optimizers()` to be\n",
      "\tdefined.\n"
     ]
    }
   ],
   "source": [
    "import logging  # import some stdlib components to control what's display\n",
    "import textwrap\n",
    "import traceback\n",
    "\n",
    "try:  # try using the LinearRegression LightningModule defined above\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)  # hide some info for now\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # we'll explain how the Trainer works in a bit\n",
    "    trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=1)\n",
    "    trainer.fit(model=model)  \n",
    "\n",
    "except pl.utilities.exceptions.MisconfigurationException as error:\n",
    "    print(\"Error:\", *textwrap.wrap(str(error), 80), sep=\"\\n\\t\")  # show the error without raising it\n",
    "\n",
    "finally:  # bring back info-level logging\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1038f7f",
   "metadata": {},
   "source": [
    "### Training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "629296fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "    xs, ys = batch  # unpack the batch\n",
    "    outs = self(xs)  # apply the model\n",
    "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
    "    return loss\n",
    "\n",
    "LinearRegression.training_step = training_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194e34e",
   "metadata": {},
   "source": [
    "### Configure optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e016ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "LinearRegression.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d20ee",
   "metadata": {},
   "source": [
    "## `pl.Trainer`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b788e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20, gpus=int(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77191634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelatedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, N=10_000):\n",
    "        self.N = N\n",
    "        self.xs = torch.randn(size=(N, 1))\n",
    "        self.ys = torch.randn_like(self.xs) + self.xs  # correlated target data: y ~ N(x, 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.xs[idx], self.ys[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "\n",
    "dataset = CorrelatedDataset()\n",
    "tdl = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50117d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs:\n",
      "tensor([[-1.4647],\n",
      "        [ 1.5247],\n",
      "        [ 0.3228],\n",
      "        [ 0.3341],\n",
      "        [ 0.4599],\n",
      "        [ 2.4992],\n",
      "        [-0.1076],\n",
      "        [-0.8604],\n",
      "        [ 0.6806],\n",
      "        [-0.2835]])\n",
      "ys:\n",
      "tensor([[-1.2774],\n",
      "        [ 2.1730],\n",
      "        [-0.1785],\n",
      "        [ 0.5804],\n",
      "        [-0.2740],\n",
      "        [ 1.4019],\n",
      "        [ 0.8990],\n",
      "        [ 0.0136],\n",
      "        [ 0.8030],\n",
      "        [-0.6972]])\n"
     ]
    }
   ],
   "source": [
    "example_xs, example_ys = next(iter(tdl))  # grabbing an example batch to print\n",
    "\n",
    "print(\"xs:\", example_xs[:10], sep=\"\\n\")\n",
    "print(\"ys:\", example_ys[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a0596dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e8ad0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
    "  .plot(x=\"x\", y=\"y\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd06d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/miniconda3/envs/fsdl-text-recognizer-2022/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/jobquiroz/full_stack_deep_learning/lab02/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 5.318972587585449\n",
      "loss after training: 5.318972587585449\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=tdl)\n",
    "\n",
    "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eabea999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
    "  .plot(x=\"x\", y=\"y\", legend=True, kind=\"scatter\", label=\"data\")\n",
    "\n",
    "inps = torch.arange(-2, 2, 0.5)[:, None]\n",
    "ax.plot(inps, model(inps).detach(), lw=2, color=\"k\", label=\"predictions\"); ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dede24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Customize every aspect of training via flags.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Trainer.__init__.__doc__.strip().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "120ccf7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Customize every aspect of training via flags.\n",
      "\n",
      "        Args:\n",
      "\n",
      "            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\")\n",
      "                as well as custom accelerator instances.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0\n",
      "                    and will be removed in v1.7.0. Please use the ``strategy`` argument instead.\n",
      "\n",
      "            accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
      "                Default: ``None``.\n",
      "\n",
      "            amp_backend: The mixed precision backend to use (\"native\" or \"apex\").\n",
      "                Default: ``'native''``.\n",
      "\n",
      "            amp_level: The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\"\n",
      "                if ``amp_backend`` is set to \"apex\".\n",
      "\n",
      "            auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
      "                trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
      "                set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
      "                To use a different key set a string instead of True with the key name.\n",
      "                Default: ``False``.\n",
      "\n",
      "            auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
      "                finder trying to find the largest batch size that fits into memory.\n",
      "                The result will be stored in self.batch_size in the LightningModule.\n",
      "                Additionally, can be set to either `power` that estimates the batch size through\n",
      "                a power search or `binsearch` that estimates the batch size through a binary search.\n",
      "                Default: ``False``.\n",
      "\n",
      "            auto_select_gpus: If enabled and ``gpus`` or ``devices`` is an integer, pick available\n",
      "                gpus automatically. This is especially useful when\n",
      "                GPUs are configured to be in \"exclusive mode\", such\n",
      "                that only one process at a time can access them.\n",
      "                Default: ``False``.\n",
      "\n",
      "            benchmark: Sets ``torch.backends.cudnn.benchmark``.\n",
      "                Defaults to ``True`` if :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      "                is ``False``. Overwrite to manually set a different value. Default: ``None``.\n",
      "\n",
      "            callbacks: Add a callback or list of callbacks.\n",
      "                Default: ``None``.\n",
      "\n",
      "            checkpoint_callback: If ``True``, enable checkpointing.\n",
      "                Default: ``None``.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``checkpoint_callback`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please consider using ``enable_checkpointing`` instead.\n",
      "\n",
      "            enable_checkpointing: If ``True``, enable checkpointing.\n",
      "                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      "                Default: ``True``.\n",
      "\n",
      "            check_val_every_n_epoch: Check val every n train epochs.\n",
      "                Default: ``1``.\n",
      "\n",
      "\n",
      "            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      "                Default: ``os.getcwd()``.\n",
      "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "\n",
      "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "                Default: ``False``.\n",
      "\n",
      "            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      "                Default: ``False``.\n",
      "\n",
      "            devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,\n",
      "                based on the accelerator type.\n",
      "\n",
      "            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      "                of train, val and test to find any bugs (ie: a sort of unit test).\n",
      "                Default: ``False``.\n",
      "\n",
      "            flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``flush_logs_every_n_steps`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please configure flushing directly in the logger instead.\n",
      "\n",
      "            gpus: Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node\n",
      "                Default: ``None``.\n",
      "\n",
      "            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      "                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      "                Default: ``None``.\n",
      "\n",
      "            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      "                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      "                be set to ``\"norm\"``.\n",
      "\n",
      "            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      "                the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are\n",
      "                provided and the `save_dir` property of that logger is not set, local files (checkpoints,\n",
      "                profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any\n",
      "                of the individual loggers.\n",
      "                Default: ``True``.\n",
      "\n",
      "            log_gpu_memory: None, 'min_max', 'all'. Might slow performance.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
      "                    Please use the ``DeviceStatsMonitor`` callback directly instead.\n",
      "\n",
      "            log_every_n_steps: How often to log within steps.\n",
      "                Default: ``50``.\n",
      "\n",
      "            prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
      "                    Please set ``prepare_data_per_node`` in ``LightningDataModule`` and/or\n",
      "                    ``LightningModule`` directly instead.\n",
      "\n",
      "            process_position: Orders the progress bar when running multiple models on same machine.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``process_position`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``process_position``\n",
      "                    directly to the Trainer's ``callbacks`` argument instead.\n",
      "\n",
      "            progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
      "                Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
      "                a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``progress_bar_refresh_rate`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``refresh_rate``\n",
      "                    directly to the Trainer's ``callbacks`` argument instead. To disable the progress bar,\n",
      "                    pass ``enable_progress_bar = False`` to the Trainer.\n",
      "\n",
      "            enable_progress_bar: Whether to enable to progress bar by default.\n",
      "                Default: ``False``.\n",
      "\n",
      "            profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      "                Default: ``None``.\n",
      "\n",
      "            overfit_batches: Overfit a fraction of training data (float) or a set number of batches (int).\n",
      "                Default: ``0.0``.\n",
      "\n",
      "            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      "                Default: ``None``.\n",
      "\n",
      "            precision: Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\n",
      "                Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
      "                Default: ``32``.\n",
      "\n",
      "            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      "                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      "                To enable infinite training, set ``max_epochs = -1``.\n",
      "\n",
      "            min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      "\n",
      "            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      "                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      "                ``max_epochs`` to ``-1``.\n",
      "\n",
      "            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      "\n",
      "            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      "                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      "                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      "                :class:`datetime.timedelta`.\n",
      "\n",
      "            num_nodes: Number of GPU nodes for distributed training.\n",
      "                Default: ``1``.\n",
      "\n",
      "            num_processes: Number of processes for distributed training with ``accelerator=\"cpu\"``.\n",
      "                Default: ``1``.\n",
      "\n",
      "            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      "                Set it to `-1` to run all batches in all validation dataloaders.\n",
      "                Default: ``2``.\n",
      "\n",
      "            reload_dataloaders_every_n_epochs: Set to a non-negative integer to reload dataloaders every n epochs.\n",
      "                Default: ``0``.\n",
      "\n",
      "            replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
      "                will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
      "                train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
      "                you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
      "\n",
      "            resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
      "                no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,\n",
      "                training will start from the beginning of the next epoch.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v2.0.\n",
      "                    Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
      "\n",
      "            strategy: Supports different training strategies with aliases\n",
      "                as well custom strategies.\n",
      "                Default: ``None``.\n",
      "\n",
      "            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      "                Default: ``False``.\n",
      "\n",
      "            terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
      "                end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    Trainer argument ``terminate_on_nan`` was deprecated in v1.5 and will be removed in 1.7.\n",
      "                    Please use ``detect_anomaly`` instead.\n",
      "\n",
      "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "                Default: ``False``.\n",
      "\n",
      "            tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on (1)\n",
      "                Default: ``None``.\n",
      "\n",
      "            ipus: How many IPUs to train on.\n",
      "                Default: ``None``.\n",
      "\n",
      "            track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using\n",
      "                Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\n",
      "                Default: ``-1``.\n",
      "\n",
      "            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      "                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      "                batches.\n",
      "                Default: ``1.0``.\n",
      "\n",
      "            enable_model_summary: Whether to enable model summarization by default.\n",
      "                Default: ``True``.\n",
      "\n",
      "            weights_summary: Prints a summary of the weights when training begins.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``weights_summary`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    To disable the summary, pass ``enable_model_summary = False`` to the Trainer.\n",
      "                    To customize the summary, pass :class:`~pytorch_lightning.callbacks.model_summary.ModelSummary`\n",
      "                    directly to the Trainer's ``callbacks`` argument.\n",
      "\n",
      "            weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
      "                for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
      "                stored in a different place than the logs written in `default_root_dir`.\n",
      "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "                Defaults to `default_root_dir`.\n",
      "\n",
      "                .. deprecated:: v1.6\n",
      "                    ``weights_save_path`` has been deprecated in v1.6 and will be removed in v1.8. Please pass\n",
      "                    ``dirpath`` directly to the :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      "                    callback.\n",
      "\n",
      "            move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
      "                This can save some gpu memory, but can make training slower. Use with attention.\n",
      "                Default: ``False``.\n",
      "\n",
      "            multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
      "                In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
      "                and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
      "                reload when reaching the minimum length of datasets.\n",
      "                Default: ``\"max_size_cycle\"``.\n",
      "\n",
      "            stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>`_.\n",
      "                Default: ``False``.\n",
      "\n",
      "                .. deprecated:: v1.5\n",
      "                    ``stochastic_weight_avg`` has been deprecated in v1.5 and will be removed in v1.7.\n",
      "                    Please pass :class:`~pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging`\n",
      "                    directly to the Trainer's ``callbacks`` argument instead.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(pl.Trainer.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c083d",
   "metadata": {},
   "source": [
    "### Training with PyLit in the FSDL Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "531ee2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __pycache__  base.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls text_recognizer/lit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5571f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightning_logs\tnotebooks  text_recognizer\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20242329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  run_experiment.py\tutil.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c00aaeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment-running framework. \n",
      "    Run an experiment.\n",
      "    Sample command:\n",
      "    ```\n",
      "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
      "    ```\n",
      "    For basic help documentation, run the command\n",
      "    ```\n",
      "    python training/run_experiment.py --help\n",
      "    ```\n",
      "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
      "    To see which command line args are available and read their documentation, provide values for those arguments\n",
      "    before invoking --help, like so:\n",
      "    ```\n",
      "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import training.run_experiment\n",
    "\n",
    "print(training.run_experiment.__doc__, training.run_experiment.main.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e7ff79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger)\r\n"
     ]
    }
   ],
   "source": [
    "# how the trainer is initialized in the training script\n",
    "!grep \"pl.Trainer.from\" training/run_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f78fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl.Trainer:\r\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\r\n",
      "                        experiment tracking. A ``True`` value uses the default\r\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\r\n",
      "                        If multiple loggers are provided and the `save_dir`\r\n",
      "                        property of that logger is not set, local files\r\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\r\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\r\n",
      "                        any of the individual loggers. Default: ``True``.\r\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\r\n",
      "                        If ``True``, enable checkpointing. Default: ``None``.\r\n",
      "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\r\n",
      "                        deprecated in v1.5 and will be removed in v1.7. Please\r\n",
      "                        consider using ``enable_checkpointing`` instead.\r\n",
      "  --enable_checkpointing [ENABLE_CHECKPOINTING]\r\n",
      "                        If ``True``, enable checkpointing. It will configure a\r\n",
      "                        default ModelCheckpoint callback if there is no user-\r\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\r\n",
      "                        ng.trainer.trainer.Trainer.callbacks`. Default:\r\n",
      "                        ``True``.\r\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\r\n",
      "                        Default path for logs and weights when no\r\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\r\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\r\n",
      "                        or 'hdfs://path/'\r\n"
     ]
    }
   ],
   "source": [
    "# displays the first few flags for controlling the Trainer from the command line\n",
    "!python training/run_experiment.py --help | grep \"pl.Trainer\" -A 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1be36c",
   "metadata": {},
   "source": [
    "### LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35fa665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
      "    advantage is consistent data splits, data preparation and transforms across models.\n",
      "\n",
      "    Example::\n",
      "\n",
      "        class MyDataModule(LightningDataModule):\n",
      "            def __init__(self):\n",
      "                super().__init__()\n",
      "            def prepare_data(self):\n",
      "                # download, split, etc...\n",
      "                # only called on 1 GPU/TPU in distributed\n",
      "            def setup(self, stage):\n",
      "                # make assignments here (val/train/test split)\n",
      "                # called on every process in DDP\n",
      "            def train_dataloader(self):\n",
      "                train_split = Dataset(...)\n",
      "                return DataLoader(train_split)\n",
      "            def val_dataloader(self):\n",
      "                val_split = Dataset(...)\n",
      "                return DataLoader(val_split)\n",
      "            def test_dataloader(self):\n",
      "                test_split = Dataset(...)\n",
      "                return DataLoader(test_split)\n",
      "            def teardown(self):\n",
      "                # clean up after fit or test\n",
      "                # called on every process in DDP\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pl.LightningDataModule.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "872312d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class CorrelatedDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
    "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
    "\n",
    "        # set some constants, like the train/val split\n",
    "        self.size = size\n",
    "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
    "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
    "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
    "\n",
    "        # under the hood, we've still got a torch Dataset\n",
    "        self.dataset = CorrelatedDataset(N=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d722a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(self, stage=None):  # prepares state that needs to be set for each GPU on each node\n",
    "    if stage == \"fit\" or stage is None:  # other stages: \"test\", \"predict\"\n",
    "        self.train_dataset = torch.utils.data.Subset(self.dataset, self.train_indices)\n",
    "        self.val_dataset = torch.utils.data.Subset(self.dataset, self.val_indices)\n",
    "\n",
    "def prepare_data(self):  # prepares state that needs to be set once per node\n",
    "    pass  # but we don't have any \"node-level\" computations\n",
    "\n",
    "\n",
    "CorrelatedDataModule.setup, CorrelatedDataModule.prepare_data = setup, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d78b2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n",
    "\n",
    "def val_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n",
    "\n",
    "CorrelatedDataModule.train_dataloader, CorrelatedDataModule.val_dataloader = train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ee007ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jobquiroz/miniconda3/envs/fsdl-text-recognizer-2022/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training: 1.7290900945663452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da3a17968642c69d844969ac7f07fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after training: 1.0625207424163818\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "dataset = datamodule.dataset\n",
    "\n",
    "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "\n",
    "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7908b70",
   "metadata": {},
   "source": [
    "## `pl.CallBack`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f50110ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BackboneFinetuning',\n",
       " 'BaseFinetuning',\n",
       " 'Callback',\n",
       " 'DeviceStatsMonitor',\n",
       " 'EarlyStopping',\n",
       " 'GPUStatsMonitor',\n",
       " 'XLAStatsMonitor',\n",
       " 'GradientAccumulationScheduler',\n",
       " 'LambdaCallback',\n",
       " 'LearningRateMonitor',\n",
       " 'ModelCheckpoint',\n",
       " 'ModelPruning',\n",
       " 'ModelSummary',\n",
       " 'BasePredictionWriter',\n",
       " 'ProgressBar',\n",
       " 'ProgressBarBase',\n",
       " 'QuantizationAwareTraining',\n",
       " 'RichModelSummary',\n",
       " 'RichProgressBar',\n",
       " 'StochasticWeightAveraging',\n",
       " 'Timer',\n",
       " 'TQDMProgressBar']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.callbacks.__all__  # builtin Callbacks from Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f34f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooks:\n",
      "\ton_after_backward, on_batch_end, on_batch_start,\n",
      "\ton_before_accelerator_backend_setup, on_before_backward,\n",
      "\ton_before_optimizer_step, on_before_zero_grad, on_configure_sharded_model,\n",
      "\ton_epoch_end, on_epoch_start, on_exception, on_fit_end, on_fit_start,\n",
      "\ton_init_end, on_init_start, on_keyboard_interrupt, on_load_checkpoint,\n",
      "\ton_predict_batch_end, on_predict_batch_start, on_predict_end,\n",
      "\ton_predict_epoch_end, on_predict_epoch_start, on_predict_start,\n",
      "\ton_pretrain_routine_end, on_pretrain_routine_start, on_sanity_check_end,\n",
      "\ton_sanity_check_start, on_save_checkpoint, on_test_batch_end,\n",
      "\ton_test_batch_start, on_test_end, on_test_epoch_end, on_test_epoch_start,\n",
      "\ton_test_start, on_train_batch_end, on_train_batch_start, on_train_end,\n",
      "\ton_train_epoch_end, on_train_epoch_start, on_train_start,\n",
      "\ton_validation_batch_end, on_validation_batch_start, on_validation_end,\n",
      "\ton_validation_epoch_end, on_validation_epoch_start, on_validation_start\n"
     ]
    }
   ],
   "source": [
    "hooks = \", \".join([method for method in dir(pl.Callback) if method.startswith(\"on_\")])\n",
    "print(\"hooks:\", *textwrap.wrap(hooks, width=80), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545144d",
   "metadata": {},
   "source": [
    "Sill Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e91114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelloWorldCallback(pl.Callback):\n",
    "\n",
    "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        print(\"👋 hello from the start of the training epoch!\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        print(\"👋 hello from the end of the validation epoch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "972d94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def on_train_batch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
    "        if random.random() > 0.995:\n",
    "            print(f\"👋 hello from inside the lucky batch, #{batch_idx}!\")\n",
    "\n",
    "\n",
    "HelloWorldCallback.on_train_batch_start = on_train_batch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "620904f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "datamodule = CorrelatedDataModule()\n",
    "\n",
    "trainer = pl.Trainer(  # we instantiate and provide the callback here, but nothing happens yet\n",
    "    max_epochs=10, gpus=int(torch.cuda.is_available()), callbacks=[HelloWorldCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c37d37c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 2     \n",
      "---------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f28579ae47420bbd8a0fcfce866188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #134!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #130!\n",
      "👋 hello from inside the lucky batch, #187!\n",
      "👋 hello from inside the lucky batch, #220!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #231!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #96!\n",
      "👋 hello from inside the lucky batch, #236!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #143!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #56!\n",
      "👋 hello from inside the lucky batch, #79!\n",
      "👋 hello from inside the lucky batch, #173!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from inside the lucky batch, #205!\n",
      "👋 hello from the start of the training epoch!\n",
      "👋 hello from the start of the training epoch!\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c867c6",
   "metadata": {},
   "source": [
    "### `torchmetrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6fcd1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics:\n",
      "\tfunctional, Accuracy, AUC, AUROC, AveragePrecision, BinnedAveragePrecision,\n",
      "\tBinnedPrecisionRecallCurve, BinnedRecallAtFixedPrecision, BLEUScore,\n",
      "\tBootStrapper, CalibrationError, CatMetric, CHRFScore, CohenKappa,\n",
      "\tConfusionMatrix, CosineSimilarity, TweedieDevianceScore, ExplainedVariance,\n",
      "\tExtendedEditDistance, F1, F1Score, FBeta, FBetaScore, HammingDistance, Hinge,\n",
      "\tHingeLoss, JaccardIndex, KLDivergence, MatthewsCorrcoef, MatthewsCorrCoef,\n",
      "\tMaxMetric, MeanAbsoluteError, MeanAbsolutePercentageError, MeanMetric,\n",
      "\tMeanSquaredError, MeanSquaredLogError, Metric, MetricCollection, MetricTracker,\n",
      "\tMinMaxMetric, MinMetric, MultioutputWrapper,\n",
      "\tMultiScaleStructuralSimilarityIndexMeasure, PearsonCorrcoef, PearsonCorrCoef,\n",
      "\tPermutationInvariantTraining, PIT, Precision, PrecisionRecallCurve, PSNR,\n",
      "\tPeakSignalNoiseRatio, R2Score, Recall, RetrievalFallOut, RetrievalHitRate,\n",
      "\tRetrievalMAP, RetrievalMRR, RetrievalNormalizedDCG, RetrievalPrecision,\n",
      "\tRetrievalRecall, RetrievalRPrecision, ROC, SacreBLEUScore, SDR,\n",
      "\tSignalDistortionRatio, ScaleInvariantSignalDistortionRatio, SI_SDR, SI_SNR,\n",
      "\tScaleInvariantSignalNoiseRatio, SignalNoiseRatio, SNR, SpearmanCorrcoef,\n",
      "\tSpearmanCorrCoef, Specificity, SQuAD, SSIM, StructuralSimilarityIndexMeasure,\n",
      "\tStatScores, SumMetric, SymmetricMeanAbsolutePercentageError,\n",
      "\tTranslationEditRate, WER, WordErrorRate, CharErrorRate, MatchErrorRate,\n",
      "\tWordInfoLost, WordInfoPreserved\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "tm_version = torchmetrics.__version__\n",
    "print(\"metrics:\", *textwrap.wrap(\", \".join(torchmetrics.__all__), width=80), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc351228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(torchmetrics.Metric, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fffbbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseLitModel.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53000e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
