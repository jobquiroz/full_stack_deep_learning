{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c83bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a058b0",
   "metadata": {},
   "source": [
    "Path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9b645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/full_stack_deep_learning/lab01/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c31dee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jobquiroz/full_stack_deep_learning/lab01/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing path\n",
    "import os\n",
    "\n",
    "os.getcwd()   # Verify where it is right now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution, go to lab directory:\n",
    "os.chdir('/home/jobquiroz/full_stack_deep_learning/lab01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea8579",
   "metadata": {},
   "source": [
    "Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d7ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist(path):\n",
    "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "    \n",
    "    if not (path / filename).exists():\n",
    "        content = requests.get(url + filename).content\n",
    "        (path / filename).open(\"wb\").write(content)\n",
    "\n",
    "    return path / filename\n",
    "\n",
    "data_path = Path(\"data\") if Path(\"data\").exists() else Path(\"../data\")\n",
    "path = data_path / \"downloaded\" / \"vector-mnist\"\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datafile = download_mnist(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749411d",
   "metadata": {},
   "source": [
    "Decompressing, deserialization and reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dcc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_mnist(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = read_mnist(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53163881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1f3be",
   "metadata": {},
   "source": [
    "From arrays to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d36715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137eea2f",
   "metadata": {},
   "source": [
    "Tensors are defined by their contents: they are big rectangular blocks of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29778435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([5, 0, 4,  ..., 8, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train, y_train, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe915c26",
   "metadata": {},
   "source": [
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eaf7326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0117, 0.0703, 0.4922, 0.6836, 0.6484,\n",
       "         0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.3672,\n",
       "         0.6641, 0.9883, 0.9883, 0.8789, 0.9883, 0.7617, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.9297, 0.9883, 0.9883, 0.9883, 0.9883, 0.3633,\n",
       "         0.3203, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8555,\n",
       "         0.9883, 0.9883, 0.7734, 0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3125, 0.4180, 0.9883, 0.0430, 0.1680,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0039, 0.9883, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.0078,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.7422, 0.2734, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367,\n",
       "         0.8789, 0.4219, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9375, 0.9883, 0.0977, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1758, 0.9883, 0.5859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3633, 0.9883,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.2500, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.7148,\n",
       "         0.9883, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1523, 0.8945, 0.9883, 0.9766, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0938, 0.8633, 0.9883,\n",
       "         0.9883, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0898, 0.8320, 0.9883, 0.9883, 0.3164, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883, 0.9883,\n",
       "         0.3125, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2148, 0.8828, 0.9883, 0.9883, 0.5195, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5312, 0.9883, 0.8281, 0.5156,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], x_train[0, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4be4dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Num dimensions\n",
    "x_train.ndim, y_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f75df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0, 0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae77c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# Shapes\n",
    "n, c = x_train.shape\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54ff8a",
   "metadata": {},
   "source": [
    "### Random handwritten digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec26463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB4ElEQVR4nO2UoavyYBTGD5fLFKdg0WB1KqJgMAwMjhWjRoPRoggmrRsWwWDS5D9gMqzJihgMEwxiEQWDCyoIC4MF2etuOCCiU9z92sd92jnPeX8823k3gD/9V6JpulgsXq9XQgghRFXVSqXyr9ByuWyaJiHEvFOj0fg9sdlsGobxDL1cLplM5jfEYDA4n8+RgtButyuKIpaDwYCiKMfQ0Wh0i4bQQCBA03Sr1cKS53lnxGq1Su6Ei4pGo+hiKYqiM6iqqphxsVhwHIfRBEFAd7PZoPVw6usNsVarhUIhALAsq9PpTKfTeDy+XC4Nw8ABWZYBAGc+UiKR0DQNo7XbbduZfr/vLCnP8z6fDwAkSXoFRUmS9FHMWCx22wzHca/GLMtysCi8hqZp7nY7v99vO8MwDL6cVCr1YL1bFAB4PB5bKEVReAcEQVitVh8lnc1mmFTX9ecgAFAqld649joejwgNh8PPbjab1TTNNM1CofApEQBOpxMu6qHvdrvz+byiKISQ8/nsgAh3j3/fZBhGUZTbBxaJRF4d/7btyrLMsiwArNfr4XAIALlcLplMer1ey7LG47Eoitvt1lnSdDp9OByef6C6rtfrdZfL5Qx3E8uyk8nkBt3v971ez3Zvf3rQD3mmeu/7OlHPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F1098F63390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-execute this cell for more samples\n",
    "import random\n",
    "\n",
    "import wandb  # just for some convenience methods that convert tensors to human-friendly datatypes\n",
    "\n",
    "import text_recognizer.metadata.mnist as metadata # metadata module holds metadata separate from data\n",
    "\n",
    "idx = random.randint(0, len(x_train))\n",
    "example = x_train[idx]\n",
    "\n",
    "print(y_train[idx])  # the label of the image\n",
    "wandb.Image(example.reshape(*metadata.DIMS)).image  # the image itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51b192",
   "metadata": {},
   "source": [
    "### From scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eaa196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82c3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x - torch.log(torch.sum(torch.exp(x), axis=1))[:, None]\n",
    "\n",
    "def model(xb: torch.Tensor) -> torch.Tensor:\n",
    "    return log_softmax(linear(xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8fa6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5763, -1.7125, -2.1168, -2.0477, -2.6708, -2.7564, -2.2827, -2.5472,\n",
      "        -2.1951, -2.6634], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a batch of inputs\n",
    "outs = model(xb)  # outputs on that batch\n",
    "\n",
    "print(outs[0], outs.shape)  # outputs on the first element of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a92ab879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee1b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "\n",
    "acc = accuracy(outs, yb)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f716d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    acc.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac3335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b656904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3123, grad_fn=<NegBackward0>) tensor(2.3026)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(outs, yb), -torch.log(torch.tensor(1 / 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82be41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(outs, yb)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aea8713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0170, -0.0144,  0.0529, -0.0093, -0.0362,  0.0252,  0.0349,  0.0198,\n",
       "        -0.0055, -0.0505])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0da87744",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5  # learning rate hyperparameter\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):  # loop over the data repeatedly\n",
    "    for ii in range((n - 1) // bs + 1):  # in batches of size bs, so roughly n / bs of them\n",
    "        start_idx = ii * bs  # we are ii batches in, each of size bs\n",
    "        end_idx = start_idx + bs  # and we want the next bs entires\n",
    "\n",
    "        # pull batches from x and from y\n",
    "        xb = x_train[start_idx:end_idx]\n",
    "        yb = y_train[start_idx:end_idx]\n",
    "\n",
    "        # run model\n",
    "        pred = model(xb)\n",
    "\n",
    "        # get loss\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        # calculate the gradients with a backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        with torch.no_grad():  # we don't want to track gradients through this part!\n",
    "            # SGD learning rule: update with negative gradient scaled by lr\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "\n",
    "            # ACHTUNG: PyTorch doesn't assume you're done with gradients\n",
    "            #          until you say so -- by explicitly \"deleting\" them,\n",
    "            #          i.e. setting the gradients to 0.\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0870853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0816, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c669b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACSUlEQVR4nO2Uu2vqYBiHYxNvpKhElP4BClEQnQRBCDgoLl10kiIUHNxEcJEWiiCog4MIIopDp4KTLi7qXlG8gFowuElaIw6CSNG+9QzhiMd6oZfpnPNM4cvve/JL3o8gyH/+ElAUJQiCIAiKoqLRaDweX6/X779ptVp2u/0TOrVanclk8vk8/Ml4PB4MBoPBYLFYAEAkEtnZiH10kSQpkUi8Xu/V1RWKotzi09PT4+NjqVRCEKTT6Tw/P4dCoevra6FQ+PLycqya2WyuVqvT6XRTqtfrlUolj8cjlUo3MZ/PV6vVuEAikeDz+QeNUql0MpkAwGg0ur291ev1er1eoVDsxOr1OsuyAMAwjM1mEwqFx2qenZ0FAgEAuLy83BtwuVz9fp8reH9/r1Qqj+m2vTiOb77j9nqlUnl7ewOAu7s7HMcxbM8wPgGGYeFwGADm83mxWPz4yD3lTibOz8+NRiOCIAzDlMtlt9ut1Wq35/ZFCILo9XrbR7XRaCSTSZIkv+XV6XS5XK5QKHS73eFwyKlZlqUo6ruVEQSRy+UXFxcajSaVSgFAOp3+AekGgUDQbDbn87lMJtu5dXpQh1gul6vVSiwW83i8H5OKRCKBQPDl7XsQi8XZbBYAyuXyMbVKpTKZTCd1GIZZLBbOeHpQ0Wh0sVjUajW/38+d9h2sVqvT6dz8n7iaBoPhmNThcDw8PHDp2WxG0zRN0+12OxgMctevr68bHcuyNzc3IpHo5JshKIpqtdpIJFIoFOAAsVjMYrFIJJLTun+VX79gWjE1JY0MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F1098F02210>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-execute this cell for more samples\n",
    "idx = random.randint(0, len(x_train))\n",
    "example = x_train[idx:idx+1]\n",
    "\n",
    "out = model(example)\n",
    "\n",
    "print(out.argmax())\n",
    "wandb.Image(example.reshape(28, 28)).image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e86a6c",
   "metadata": {},
   "source": [
    "### torch.nn components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22f7ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb4eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0816, grad_fn=<NllLossBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))  # should be unchanged from above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69b801",
   "metadata": {},
   "source": [
    "Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28fb7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MNISTLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # the nn.Module.__init__ method does import setup, so this is mandatory\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2960394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0580,  0.1888, -0.0354, -0.5416,  0.0383, -0.5461,  0.1413, -0.9100,\n",
      "          0.1955, -0.2180],\n",
      "        [-0.5383, -0.2115, -0.2720, -0.7730, -0.1205, -0.2894,  0.1344, -0.3800,\n",
      "         -0.3505,  0.3573],\n",
      "        [-0.4333,  0.1738, -0.2802, -0.0995, -0.0609, -0.4295,  0.2541, -0.5983,\n",
      "          0.1048, -0.0539],\n",
      "        [-0.5309, -0.2772, -0.2097, -0.3301, -0.1478, -0.1996,  0.2732, -0.6092,\n",
      "         -0.1398,  0.3469]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0056,  0.0082,  0.0088,  0.0080,  0.0077],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0374, -0.0024,  0.0528,  0.0571, -0.1302],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0294,  0.0219, -0.0414,  0.0165, -0.0402],\n",
      "        [ 0.0189,  0.0312,  0.0216,  0.0219, -0.0464],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0790,  0.0268,  0.0466,  0.0539, -0.0215],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0978,  0.0326,  0.0037,  0.0266, -0.0139],\n",
      "        [-0.0630,  0.0722,  0.0560,  0.0604, -0.1035],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0206,  0.0296,  0.0296,  0.0297, -0.0430],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0553,  0.0087,  0.0043,  0.0065,  0.0064],\n",
      "        [ 0.0195,  0.0301, -0.0579,  0.0255, -0.0403],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0387,  0.0574, -0.0183,  0.0525, -0.0692],\n",
      "        [ 0.0063,  0.0074, -0.0556,  0.0059,  0.0062],\n",
      "        [-0.0273,  0.0055,  0.0025,  0.0037,  0.0040],\n",
      "        [-0.0216, -0.0058, -0.0512,  0.0592,  0.0038],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0256,  0.0039, -0.0423,  0.0664, -0.0818],\n",
      "        [-0.0557,  0.0082,  0.0092,  0.0080,  0.0090],\n",
      "        [-0.0013,  0.0001,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0570,  0.0471, -0.0032,  0.0474, -0.1073],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0928,  0.0495, -0.0080,  0.0605, -0.0613],\n",
      "        [-0.0557,  0.0081,  0.0091,  0.0079,  0.0089],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1139,  0.0354,  0.0803,  0.0905, -0.0968],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0157,  0.0184,  0.0187,  0.0169, -0.0420],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0006,  0.0014,  0.0010,  0.0012,  0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def forward(self, xb: torch.Tensor) -> torch.Tensor:\n",
    "    return xb @ self.weights + self.bias\n",
    "\n",
    "MNISTLogistic.forward = forward\n",
    "\n",
    "model = MNISTLogistic()  # instantiated as an object\n",
    "print(model(xb)[:4])  # callable like a function\n",
    "loss = loss_func(model(xb), yb)  # composable like a function\n",
    "loss.backward()  # we can still take gradients through it\n",
    "print(model.weights.grad[::17,::2])  # and they show up in the .grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca703d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0172, -0.0556,  0.0517,  ..., -0.0461, -0.0101, -0.0111],\n",
      "        [-0.0247,  0.0041,  0.0099,  ...,  0.0022,  0.0179, -0.0108],\n",
      "        [ 0.0060,  0.0657, -0.0154,  ...,  0.0095,  0.0129, -0.0090],\n",
      "        ...,\n",
      "        [-0.0244, -0.0197, -0.0175,  ..., -0.0078,  0.0224, -0.0744],\n",
      "        [-0.0077, -0.0143,  0.0279,  ..., -0.0035, -0.0164,  0.0105],\n",
      "        [ 0.0871, -0.0206, -0.0215,  ..., -0.0271,  0.0016,  0.0516]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(*list(model.parameters()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f98ed2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for ii in range((n - 1) // bs + 1):\n",
    "            start_idx = ii * bs\n",
    "            end_idx = start_idx + bs\n",
    "            xb = x_train[start_idx:end_idx]\n",
    "            yb = y_train[start_idx:end_idx]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():  # finds params automatically\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23a63afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98b4de",
   "metadata": {},
   "source": [
    "## More refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bf2413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.nn.Modules:\n",
      "\tModule, Identity, Linear, Conv1d, Conv2d, Conv3d, ConvTranspose1d,\n",
      "\tConvTranspose2d, ConvTranspose3d, Threshold, ReLU, Hardtanh, ReLU6,\n",
      "\tSigmoid, Tanh, Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GLU,\n",
      "\tGELU, Hardshrink, LeakyReLU, LogSigmoid, Softplus, Softshrink,\n",
      "\tMultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU,\n",
      "\tL1Loss, NLLLoss, KLDivLoss, MSELoss, BCELoss, BCEWithLogitsLoss,\n",
      "\tNLLLoss2d, PoissonNLLLoss, CosineEmbeddingLoss, CTCLoss,\n",
      "\tHingeEmbeddingLoss, MarginRankingLoss, MultiLabelMarginLoss,\n",
      "\tMultiLabelSoftMarginLoss, MultiMarginLoss, SmoothL1Loss,\n",
      "\tGaussianNLLLoss, HuberLoss, SoftMarginLoss, CrossEntropyLoss,\n",
      "\tContainer, Sequential, ModuleList, ModuleDict, ParameterList,\n",
      "\tParameterDict, AvgPool1d, AvgPool2d, AvgPool3d, MaxPool1d, MaxPool2d,\n",
      "\tMaxPool3d, MaxUnpool1d, MaxUnpool2d, MaxUnpool3d, FractionalMaxPool2d,\n",
      "\tFractionalMaxPool3d, LPPool1d, LPPool2d, LocalResponseNorm,\n",
      "\tBatchNorm1d, BatchNorm2d, BatchNorm3d, InstanceNorm1d, InstanceNorm2d,\n",
      "\tInstanceNorm3d, LayerNorm, GroupNorm, SyncBatchNorm, Dropout,\n",
      "\tDropout1d, Dropout2d, Dropout3d, AlphaDropout, FeatureAlphaDropout,\n",
      "\tReflectionPad1d, ReflectionPad2d, ReflectionPad3d, ReplicationPad2d,\n",
      "\tReplicationPad1d, ReplicationPad3d, CrossMapLRN2d, Embedding,\n",
      "\tEmbeddingBag, RNNBase, RNN, LSTM, GRU, RNNCellBase, RNNCell, LSTMCell,\n",
      "\tGRUCell, PixelShuffle, PixelUnshuffle, Upsample, UpsamplingNearest2d,\n",
      "\tUpsamplingBilinear2d, PairwiseDistance, AdaptiveMaxPool1d,\n",
      "\tAdaptiveMaxPool2d, AdaptiveMaxPool3d, AdaptiveAvgPool1d,\n",
      "\tAdaptiveAvgPool2d, AdaptiveAvgPool3d, TripletMarginLoss, ZeroPad2d,\n",
      "\tConstantPad1d, ConstantPad2d, ConstantPad3d, Bilinear,\n",
      "\tCosineSimilarity, Unfold, Fold, AdaptiveLogSoftmaxWithLoss,\n",
      "\tTransformerEncoder, TransformerDecoder, TransformerEncoderLayer,\n",
      "\tTransformerDecoderLayer, Transformer, LazyLinear, LazyConv1d,\n",
      "\tLazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d,\n",
      "\tLazyConvTranspose3d, LazyBatchNorm1d, LazyBatchNorm2d,\n",
      "\tLazyBatchNorm3d, LazyInstanceNorm1d, LazyInstanceNorm2d,\n",
      "\tLazyInstanceNorm3d, Flatten, Unflatten, Hardsigmoid, Hardswish, SiLU,\n",
      "\tMish, TripletMarginWithDistanceLoss, ChannelShuffle\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"torch.nn.Modules:\", *textwrap.wrap(\", \".join(torch.nn.modules.__all__)), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4d8294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)  # pytorch finds the nn.Parameters inside this nn.Module\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)  # call nn.Linear.forward here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "500e82ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3471, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "print(loss_func(model(xb), yb))  # loss is still close to 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3154201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee2d06e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "406e9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0192, -0.0186,  0.0115,  ...,  0.0354, -0.0024, -0.0329],\n",
      "        [ 0.0021,  0.0258,  0.0182,  ..., -0.0221, -0.0090,  0.0193],\n",
      "        [ 0.0167, -0.0221,  0.0237,  ...,  0.0076,  0.0221, -0.0029],\n",
      "        ...,\n",
      "        [-0.0296,  0.0267, -0.0296,  ..., -0.0333, -0.0013, -0.0033],\n",
      "        [-0.0203,  0.0292, -0.0109,  ..., -0.0344,  0.0138, -0.0261],\n",
      "        [ 0.0023, -0.0327,  0.0296,  ..., -0.0318, -0.0311, -0.0213]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0024, -0.0078, -0.0356,  0.0324,  0.0089, -0.0323, -0.0211, -0.0103,\n",
      "         0.0016,  0.0103], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(*list(model.parameters()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dbfc375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0192, -0.0186,  0.0115,  ...,  0.0354, -0.0024, -0.0329],\n",
       "        [ 0.0021,  0.0258,  0.0182,  ..., -0.0221, -0.0090,  0.0193],\n",
       "        [ 0.0167, -0.0221,  0.0237,  ...,  0.0076,  0.0221, -0.0029],\n",
       "        ...,\n",
       "        [-0.0296,  0.0267, -0.0296,  ..., -0.0333, -0.0013, -0.0033],\n",
       "        [-0.0203,  0.0292, -0.0109,  ..., -0.0344,  0.0138, -0.0261],\n",
       "        [ 0.0023, -0.0327,  0.0296,  ..., -0.0318, -0.0311, -0.0213]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522df22",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b2adb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
    "    return optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "476e1d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training:\n",
      "\ttensor(2.4095, grad_fn=<NllLossBackward0>)\n",
      "after training:\n",
      "\ttensor(0.8542, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "opt = configure_optimizer(model)\n",
    "\n",
    "print(\"before training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for ii in range((n - 1) // bs + 1):\n",
    "        start_idx = ii * bs\n",
    "        end_idx = start_idx + bs\n",
    "        xb = x_train[start_idx:end_idx]\n",
    "        yb = y_train[start_idx:end_idx]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(\"after training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d20bad",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "372000fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.data.util import BaseDataset\n",
    "\n",
    "train_ds = BaseDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10cbcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseDataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ac4f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8333, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "opt = configure_optimizer(model)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for ii in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[ii * bs: ii * bs + bs]  # xb and yb in one line!\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01874901",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9e80e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = BaseDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a35d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self: nn.Module, train_dataloader: DataLoader):\n",
    "    opt = configure_optimizer(self)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dataloader:\n",
    "            pred = self(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "MNISTLogistic.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64db4a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6764, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "odel = MNISTLogistic()\n",
    "\n",
    "model.fit(train_dataloader)\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e887660",
   "metadata": {},
   "source": [
    "### Different Model... MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feeda425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.models.mlp import MLP\n",
    "\n",
    "MLP.fit = fit  # attach our fitting loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19ec01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56f9a314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dims': (784,),\n",
       " 'mapping': {0: '0',\n",
       "  1: '1',\n",
       "  2: '2',\n",
       "  3: '3',\n",
       "  4: '4',\n",
       "  5: '5',\n",
       "  6: '6',\n",
       "  7: '7',\n",
       "  8: '8',\n",
       "  9: '9'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_to_9 = list(range(10))\n",
    "\n",
    "data_config = {\"input_dims\": (784,), \"mapping\": {digit: str(digit) for digit in digits_to_9}}\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cf35528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(data_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47a032ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 784])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1de0c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training: tensor(2.3112, grad_fn=<NllLossBackward0>)\n",
      "after training: tensor(0.2160, grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 19.5 s, sys: 38.2 ms, total: 19.6 s\n",
      "Wall time: 9.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"before training:\", loss_func(model(xb), yb))\n",
    "\n",
    "train_ds = BaseDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=bs)\n",
    "fit(model, train_dataloader)\n",
    "\n",
    "print(\"after training:\", loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf1644",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7171e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 21 02:43:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8    14W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"☹️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a7ddf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func(model(xb.to(device)), yb.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33b1df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_device(tensor):\n",
    "    return tensor.to(device)\n",
    "\n",
    "train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7f0d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 5.73 s, sys: 299 ms, total: 6.03 s\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MLP(data_config)\n",
    "model.to(device)\n",
    "\n",
    "model.fit(train_dataloader)\n",
    "\n",
    "print(loss_func(model(push_to_device(xb)), push_to_device(yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd63b3",
   "metadata": {},
   "source": [
    "## Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3611efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule:\n",
    "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "    \n",
    "    def __init__(self, dir, bs=32):\n",
    "        self.dir = dir\n",
    "        self.bs = bs\n",
    "        self.path = self.dir / self.filename\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not (self.path).exists():\n",
    "            content = requests.get(self.url + self.filename).content\n",
    "            self.path.open(\"wb\").write(content)\n",
    "\n",
    "    def setup(self):\n",
    "        with gzip.open(self.path, \"rb\") as f:\n",
    "            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "        x_train, y_train, x_valid, y_valid = map(\n",
    "            torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "            )\n",
    "        \n",
    "        self.train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
    "        self.valid_ds = BaseDataset(x_valid, y_valid, transform=push_to_device, target_transform=push_to_device)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.valid_ds, batch_size=2 * self.bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b40b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self: nn.Module, datamodule):\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()\n",
    "\n",
    "    val_dataloader = datamodule.val_dataloader()\n",
    "    \n",
    "    self.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
    "\n",
    "    print(\"before start of training:\", valid_loss / len(val_dataloader))\n",
    "\n",
    "    opt = configure_optimizer(self)\n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    for epoch in range(epochs):\n",
    "        self.train()\n",
    "        for xb, yb in train_dataloader:\n",
    "            pred = self(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
    "\n",
    "        print(epoch, valid_loss / len(val_dataloader))\n",
    "\n",
    "\n",
    "MNISTLogistic.fit = fit\n",
    "MLP.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cefffed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before start of training: tensor(2.3024, device='cuda:0')\n",
      "0 tensor(0.1682, device='cuda:0')\n",
      "1 tensor(0.1100, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = MLP(data_config)\n",
    "model.to(device)\n",
    "\n",
    "datamodule = MNISTDataModule(dir=path, bs=32)\n",
    "\n",
    "model.fit(datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e9116e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9685, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader = datamodule.val_dataloader()\n",
    "valid_acc = sum(accuracy(model(xb), yb) for xb, yb in val_dataloader) / len(val_dataloader)\n",
    "valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f0eff",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4898f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before start of training: tensor(2.3049, device='cuda:0')\n",
      "0 tensor(0.1499, device='cuda:0')\n",
      "1 tensor(0.1110, device='cuda:0')\n",
      "CPU times: user 9.77 s, sys: 585 ms, total: 10.4 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from argparse import Namespace  # you'll need this\n",
    "\n",
    "#args = None  # edit this\n",
    "args = {'fc1': 512, 'fc2': 256, 'fc_dropout': 0.25}\n",
    "args = Namespace(**args)\n",
    "\n",
    "epochs = 2  # used in fit\n",
    "bs = 32  # used by the DataModule\n",
    "\n",
    "\n",
    "# used in fit, play around with this if you'd like\n",
    "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
    "    return optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "model = MLP(data_config, args=args)\n",
    "model.to(device)\n",
    "\n",
    "datamodule = MNISTDataModule(dir=path, bs=bs)\n",
    "\n",
    "model.fit(datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8384d9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9664, device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader = datamodule.val_dataloader()\n",
    "valid_acc = sum(accuracy(model(xb), yb) for xb, yb in val_dataloader) / len(val_dataloader)\n",
    "valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727accc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
