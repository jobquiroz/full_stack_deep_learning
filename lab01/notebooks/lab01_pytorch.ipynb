{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f5bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf61525",
   "metadata": {},
   "source": [
    "Path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd198f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/full_stack_deep_learning/lab01/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e7534c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jobquiroz/full_stack_deep_learning/lab01/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing path\n",
    "import os\n",
    "\n",
    "os.getcwd()   # Verify where it is right now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fc0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution, go to lab directory:\n",
    "os.chdir('/home/jobquiroz/full_stack_deep_learning/lab01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5e075",
   "metadata": {},
   "source": [
    "Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75b56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist(path):\n",
    "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "    \n",
    "    if not (path / filename).exists():\n",
    "        content = requests.get(url + filename).content\n",
    "        (path / filename).open(\"wb\").write(content)\n",
    "\n",
    "    return path / filename\n",
    "\n",
    "data_path = Path(\"data\") if Path(\"data\").exists() else Path(\"../data\")\n",
    "path = data_path / \"downloaded\" / \"vector-mnist\"\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datafile = download_mnist(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43d723",
   "metadata": {},
   "source": [
    "Decompressing, deserialization and reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b73a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_mnist(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = read_mnist(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cdf1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba5942",
   "metadata": {},
   "source": [
    "From arrays to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43aa028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407efbdc",
   "metadata": {},
   "source": [
    "Tensors are defined by their contents: they are big rectangular blocks of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f012cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([5, 0, 4,  ..., 8, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train, y_train, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91667801",
   "metadata": {},
   "source": [
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dabe1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5),\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0117, 0.0703, 0.4922, 0.6836, 0.6484,\n",
       "         0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.3672,\n",
       "         0.6641, 0.9883, 0.9883, 0.8789, 0.9883, 0.7617, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.9297, 0.9883, 0.9883, 0.9883, 0.9883, 0.3633,\n",
       "         0.3203, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8555,\n",
       "         0.9883, 0.9883, 0.7734, 0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3125, 0.4180, 0.9883, 0.0430, 0.1680,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0039, 0.9883, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.0078,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.7422, 0.2734, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367,\n",
       "         0.8789, 0.4219, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9375, 0.9883, 0.0977, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1758, 0.9883, 0.5859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3633, 0.9883,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.2500, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.7148,\n",
       "         0.9883, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1523, 0.8945, 0.9883, 0.9766, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0938, 0.8633, 0.9883,\n",
       "         0.9883, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0898, 0.8320, 0.9883, 0.9883, 0.3164, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883, 0.9883,\n",
       "         0.3125, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2148, 0.8828, 0.9883, 0.9883, 0.5195, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5312, 0.9883, 0.8281, 0.5156,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], x_train[0, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b5d197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Num dimensions\n",
    "x_train.ndim, y_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27215bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0, 0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21b296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# Shapes\n",
    "n, c = x_train.shape\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be796b35",
   "metadata": {},
   "source": [
    "### Random handwritten digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676ef8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB10lEQVR4nO3TO6viQBgG4GGyYiSFgUAgInhBK63tbBVsRBHrIPgLLETBIhZBxMZCGwvxgr2k8CcIVgELLwhiIViE3Gy0GNxiQHLOoosJyzbnLb/JPLzMTAD4yf8KhNDtdpdKpdPp9Hg8KpWKy+VyJHo8nlqthr5mNpu9+v7Xe44giHA4LElSNBoFAFyv191uh7n9fm+zYzwex71ut1un0/H7/dZVlmU5jvN6vR+IgUBgs9lgUZIk65LP5xNFUdM0wzAEQfgA7fV6uGaz2XwOKYrKZDKKoui6vt1u0+n0B6IVLRaLeBIKhUajEUJI07R2u/1q418uyppIJDKZTBKJhGmaPM/P5/PPOuLU63XcdLFY5HK5y+WCEFJVNZvN2uFwGIY5n8/WtynLsiMRJ5lMWtFyuexUBADQNC3L8hPVdb3b7TpFg8Hgt79T1/VCoeAIFUURX04+n3++MMMwHLmDwQAhNJ1OAQAkSfb7feyuViuapu2ILMuapml9/CRJLpdL7MZisVcb4RuUIAiKoqyTVCrFMAwA4Hg8qqpqpynHcbjU4XBotVqKotzvdzypVqt2RAAAhHA4HKI/0mg0CIKwiQIAGIYRBGG9XmNuPB7zPA/hu0P7yT/JbzjAQcuKQbqgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7EFFF59CA350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-execute this cell for more samples\n",
    "import random\n",
    "\n",
    "import wandb  # just for some convenience methods that convert tensors to human-friendly datatypes\n",
    "\n",
    "import text_recognizer.metadata.mnist as metadata # metadata module holds metadata separate from data\n",
    "\n",
    "idx = random.randint(0, len(x_train))\n",
    "example = x_train[idx]\n",
    "\n",
    "print(y_train[idx])  # the label of the image\n",
    "wandb.Image(example.reshape(*metadata.DIMS)).image  # the image itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a646b4",
   "metadata": {},
   "source": [
    "### From scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a10d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6e91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4751c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x - torch.log(torch.sum(torch.exp(x), axis=1))[:, None]\n",
    "\n",
    "def model(xb: torch.Tensor) -> torch.Tensor:\n",
    "    return log_softmax(linear(xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24672a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5404, -2.6016, -2.5248, -2.4114, -2.5581, -2.2841, -1.8198, -2.2158,\n",
      "        -2.1511, -2.2032], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a batch of inputs\n",
    "outs = model(xb)  # outputs on that batch\n",
    "\n",
    "print(outs[0], outs.shape)  # outputs on the first element of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48058205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f835f18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1406)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "\n",
    "acc = accuracy(outs, yb)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2554bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    acc.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9934d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae124cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3945, grad_fn=<NegBackward0>) tensor(2.3026)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(outs, yb), -torch.log(torch.tensor(1 / 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c07defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(outs, yb)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "266653f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0381, -0.0422,  0.0153, -0.0230, -0.0165,  0.0292,  0.0389,  0.0122,\n",
       "         0.0483, -0.0241])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9ed18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5  # learning rate hyperparameter\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):  # loop over the data repeatedly\n",
    "    for ii in range((n - 1) // bs + 1):  # in batches of size bs, so roughly n / bs of them\n",
    "        start_idx = ii * bs  # we are ii batches in, each of size bs\n",
    "        end_idx = start_idx + bs  # and we want the next bs entires\n",
    "\n",
    "        # pull batches from x and from y\n",
    "        xb = x_train[start_idx:end_idx]\n",
    "        yb = y_train[start_idx:end_idx]\n",
    "\n",
    "        # run model\n",
    "        pred = model(xb)\n",
    "\n",
    "        # get loss\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        # calculate the gradients with a backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        with torch.no_grad():  # we don't want to track gradients through this part!\n",
    "            # SGD learning rule: update with negative gradient scaled by lr\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "\n",
    "            # ACHTUNG: PyTorch doesn't assume you're done with gradients\n",
    "            #          until you say so -- by explicitly \"deleting\" them,\n",
    "            #          i.e. setting the gradients to 0.\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bccc2ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0820, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1bdb334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB4UlEQVR4nO2UMa+qMBiGibk6uNjFgcnJGBZZnO2oDGyOxsVEQwKjwcXB/+DiItHwE5z1F7g4gJJAoguYKIkuDrZwBmIPR0A9jvfeZ2q/vn1SPtJS1H/+Klqt1nA4xBj7vo8xdhxHkiQI4edGwzCu1ytCCCGEMUZ3XNet1+ufGAVBCIsexrZtVyqVhy2pl9JSqfRkNZ/Pdzqd30kBACzLPhRns9lutyNTnuejmWdMp9OHT7Ysi6bpcrlsmiapi6L4rpHjOMdxwtLRaMQwTLDKMEyS9E+SEUKoqmoulwsXN5uNruvBmAyixEshhIvFgkxTqZTnedFYUj1GynGcqqq+7wdTTdP2+72maePx+Hg8hpOe55HYM0gfA1arVaFQiMYymUy/3w96mpT5ZrvdohC1Wi02ViwWyQ9MynxjGAa+I8tyUqzdbgfvwHw+f2EMn3QymaTT6diMIAiXywUhpCgKTdPRwI8bBQAgItM0b7dbdAMAoNfrZbNZiqIsy7Jt+8UxJUki3YwNNBqN5XKJELJt+91bBCF0XTdoaLVaJXWWZSVJIu+poiiDweAtYwDp6el02t45HA5Bcb1ey7Kc1OtEut2uaZroJxjj8/ms63qz2fydjgAAEEUxLBVFkef5D3X/Hl97f5pNqe0R8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7EFFF5960690>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-execute this cell for more samples\n",
    "idx = random.randint(0, len(x_train))\n",
    "example = x_train[idx:idx+1]\n",
    "\n",
    "out = model(example)\n",
    "\n",
    "print(out.argmax())\n",
    "wandb.Image(example.reshape(28, 28)).image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f9ae",
   "metadata": {},
   "source": [
    "### torch.nn components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39863839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4af99fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0820, grad_fn=<NllLossBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))  # should be unchanged from above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c7b2d",
   "metadata": {},
   "source": [
    "Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49e689b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MNISTLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # the nn.Module.__init__ method does import setup, so this is mandatory\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10173ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1158, -0.4842,  0.1280, -0.4428, -0.0124,  0.2508, -0.3444, -0.3650,\n",
      "          0.0300, -0.0837],\n",
      "        [-0.3843, -0.0834,  0.0929, -0.4112,  0.5065,  0.7480, -0.4524, -1.1187,\n",
      "         -0.3566, -0.3108],\n",
      "        [ 0.0712, -0.6250,  0.1582, -0.2846, -0.1893, -0.1965, -0.0759, -0.2788,\n",
      "          0.1486, -0.4321],\n",
      "        [ 0.4173, -0.4888,  0.3655,  0.0792, -0.0726,  0.1883,  0.0698, -0.9419,\n",
      "         -0.2531, -0.3400]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0052,  0.0111,  0.0120,  0.0045,  0.0068],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0342,  0.0067,  0.0665,  0.0412, -0.1303],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0331,  0.0222, -0.0357,  0.0121, -0.0412],\n",
      "        [ 0.0153,  0.0355,  0.0303,  0.0163, -0.0470],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0680,  0.0346,  0.0511,  0.0374, -0.0232],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0979,  0.0330,  0.0068,  0.0183, -0.0143],\n",
      "        [-0.0669,  0.0821,  0.0728,  0.0467, -0.1054],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0215,  0.0353,  0.0361,  0.0212, -0.0431],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0588,  0.0093,  0.0096,  0.0038,  0.0066],\n",
      "        [ 0.0232,  0.0269, -0.0553,  0.0179, -0.0408],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0443,  0.0596, -0.0073,  0.0386, -0.0703],\n",
      "        [ 0.0061,  0.0066, -0.0573,  0.0048,  0.0051],\n",
      "        [-0.0292,  0.0057,  0.0058,  0.0023,  0.0038],\n",
      "        [-0.0108, -0.0043, -0.0455,  0.0415,  0.0011],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0163,  0.0064, -0.0365,  0.0466, -0.0848],\n",
      "        [-0.0555,  0.0117,  0.0085,  0.0077,  0.0091],\n",
      "        [-0.0013,  0.0002,  0.0001,  0.0002,  0.0002],\n",
      "        [-0.0523,  0.0559,  0.0090,  0.0366, -0.1070],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0937,  0.0596,  0.0076,  0.0452, -0.0666],\n",
      "        [-0.0556,  0.0116,  0.0085,  0.0076,  0.0090],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1077,  0.0469,  0.1001,  0.0654, -0.0981],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0138,  0.0194,  0.0220,  0.0128, -0.0415],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0012,  0.0009,  0.0007,  0.0006,  0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def forward(self, xb: torch.Tensor) -> torch.Tensor:\n",
    "    return xb @ self.weights + self.bias\n",
    "\n",
    "MNISTLogistic.forward = forward\n",
    "\n",
    "model = MNISTLogistic()  # instantiated as an object\n",
    "print(model(xb)[:4])  # callable like a function\n",
    "loss = loss_func(model(xb), yb)  # composable like a function\n",
    "loss.backward()  # we can still take gradients through it\n",
    "print(model.weights.grad[::17,::2])  # and they show up in the .grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a749682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0177, -0.0369, -0.0078,  ..., -0.0306,  0.0027,  0.0455],\n",
      "        [ 0.0113, -0.0143,  0.0618,  ...,  0.0315, -0.0494, -0.0522],\n",
      "        [ 0.0348,  0.0339, -0.0554,  ..., -0.0474, -0.0362,  0.0511],\n",
      "        ...,\n",
      "        [ 0.0383,  0.0019, -0.0314,  ...,  0.0505, -0.0211, -0.0347],\n",
      "        [ 0.0088, -0.0135, -0.0186,  ...,  0.0070,  0.0372, -0.0421],\n",
      "        [-0.0112, -0.0248,  0.0078,  ..., -0.0134,  0.1023,  0.0508]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(*list(model.parameters()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e63c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for ii in range((n - 1) // bs + 1):\n",
    "            start_idx = ii * bs\n",
    "            end_idx = start_idx + bs\n",
    "            xb = x_train[start_idx:end_idx]\n",
    "            yb = y_train[start_idx:end_idx]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():  # finds params automatically\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f044d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc55af",
   "metadata": {},
   "source": [
    "## More refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c9ba006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.nn.Modules:\n",
      "\tModule, Identity, Linear, Conv1d, Conv2d, Conv3d, ConvTranspose1d,\n",
      "\tConvTranspose2d, ConvTranspose3d, Threshold, ReLU, Hardtanh, ReLU6,\n",
      "\tSigmoid, Tanh, Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GLU,\n",
      "\tGELU, Hardshrink, LeakyReLU, LogSigmoid, Softplus, Softshrink,\n",
      "\tMultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU,\n",
      "\tL1Loss, NLLLoss, KLDivLoss, MSELoss, BCELoss, BCEWithLogitsLoss,\n",
      "\tNLLLoss2d, PoissonNLLLoss, CosineEmbeddingLoss, CTCLoss,\n",
      "\tHingeEmbeddingLoss, MarginRankingLoss, MultiLabelMarginLoss,\n",
      "\tMultiLabelSoftMarginLoss, MultiMarginLoss, SmoothL1Loss,\n",
      "\tGaussianNLLLoss, HuberLoss, SoftMarginLoss, CrossEntropyLoss,\n",
      "\tContainer, Sequential, ModuleList, ModuleDict, ParameterList,\n",
      "\tParameterDict, AvgPool1d, AvgPool2d, AvgPool3d, MaxPool1d, MaxPool2d,\n",
      "\tMaxPool3d, MaxUnpool1d, MaxUnpool2d, MaxUnpool3d, FractionalMaxPool2d,\n",
      "\tFractionalMaxPool3d, LPPool1d, LPPool2d, LocalResponseNorm,\n",
      "\tBatchNorm1d, BatchNorm2d, BatchNorm3d, InstanceNorm1d, InstanceNorm2d,\n",
      "\tInstanceNorm3d, LayerNorm, GroupNorm, SyncBatchNorm, Dropout,\n",
      "\tDropout1d, Dropout2d, Dropout3d, AlphaDropout, FeatureAlphaDropout,\n",
      "\tReflectionPad1d, ReflectionPad2d, ReflectionPad3d, ReplicationPad2d,\n",
      "\tReplicationPad1d, ReplicationPad3d, CrossMapLRN2d, Embedding,\n",
      "\tEmbeddingBag, RNNBase, RNN, LSTM, GRU, RNNCellBase, RNNCell, LSTMCell,\n",
      "\tGRUCell, PixelShuffle, PixelUnshuffle, Upsample, UpsamplingNearest2d,\n",
      "\tUpsamplingBilinear2d, PairwiseDistance, AdaptiveMaxPool1d,\n",
      "\tAdaptiveMaxPool2d, AdaptiveMaxPool3d, AdaptiveAvgPool1d,\n",
      "\tAdaptiveAvgPool2d, AdaptiveAvgPool3d, TripletMarginLoss, ZeroPad2d,\n",
      "\tConstantPad1d, ConstantPad2d, ConstantPad3d, Bilinear,\n",
      "\tCosineSimilarity, Unfold, Fold, AdaptiveLogSoftmaxWithLoss,\n",
      "\tTransformerEncoder, TransformerDecoder, TransformerEncoderLayer,\n",
      "\tTransformerDecoderLayer, Transformer, LazyLinear, LazyConv1d,\n",
      "\tLazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d,\n",
      "\tLazyConvTranspose3d, LazyBatchNorm1d, LazyBatchNorm2d,\n",
      "\tLazyBatchNorm3d, LazyInstanceNorm1d, LazyInstanceNorm2d,\n",
      "\tLazyInstanceNorm3d, Flatten, Unflatten, Hardsigmoid, Hardswish, SiLU,\n",
      "\tMish, TripletMarginWithDistanceLoss, ChannelShuffle\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"torch.nn.Modules:\", *textwrap.wrap(\", \".join(torch.nn.modules.__all__)), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad9b54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)  # pytorch finds the nn.Parameters inside this nn.Module\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)  # call nn.Linear.forward here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eaa079f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2877, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "print(loss_func(model(xb), yb))  # loss is still close to 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcbf3b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a83938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6238b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0194,  0.0059,  0.0076,  ...,  0.0052, -0.0344, -0.0054],\n",
      "        [-0.0152,  0.0074,  0.0338,  ..., -0.0222, -0.0286,  0.0147],\n",
      "        [ 0.0098,  0.0208,  0.0176,  ..., -0.0285, -0.0227,  0.0185],\n",
      "        ...,\n",
      "        [-0.0013,  0.0180, -0.0030,  ..., -0.0355,  0.0187,  0.0113],\n",
      "        [-0.0308, -0.0090, -0.0044,  ..., -0.0256, -0.0192,  0.0015],\n",
      "        [ 0.0255,  0.0167, -0.0049,  ..., -0.0022,  0.0129, -0.0018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0136,  0.0260,  0.0141,  0.0105,  0.0312, -0.0033, -0.0323, -0.0283,\n",
      "         0.0302, -0.0045], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(*list(model.parameters()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e94ace2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0194,  0.0059,  0.0076,  ...,  0.0052, -0.0344, -0.0054],\n",
       "        [-0.0152,  0.0074,  0.0338,  ..., -0.0222, -0.0286,  0.0147],\n",
       "        [ 0.0098,  0.0208,  0.0176,  ..., -0.0285, -0.0227,  0.0185],\n",
       "        ...,\n",
       "        [-0.0013,  0.0180, -0.0030,  ..., -0.0355,  0.0187,  0.0113],\n",
       "        [-0.0308, -0.0090, -0.0044,  ..., -0.0256, -0.0192,  0.0015],\n",
       "        [ 0.0255,  0.0167, -0.0049,  ..., -0.0022,  0.0129, -0.0018]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4512df",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c116295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
    "    return optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34f54435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training:\n",
      "\ttensor(2.3263, grad_fn=<NllLossBackward0>)\n",
      "after training:\n",
      "\ttensor(0.8633, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "opt = configure_optimizer(model)\n",
    "\n",
    "print(\"before training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for ii in range((n - 1) // bs + 1):\n",
    "        start_idx = ii * bs\n",
    "        end_idx = start_idx + bs\n",
    "        xb = x_train[start_idx:end_idx]\n",
    "        yb = y_train[start_idx:end_idx]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(\"after training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7975a6",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2d5aa3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_recognizer.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-cce498c42a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'text_recognizer.data'"
     ]
    }
   ],
   "source": [
    "from text_recognizer.data.util import BaseDataset\n",
    "\n",
    "\n",
    "train_ds = BaseDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c80d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
