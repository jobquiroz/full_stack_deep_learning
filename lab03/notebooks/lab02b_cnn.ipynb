{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8eed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/full_stack_deep_learning/lab02/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bdafd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jobquiroz/full_stack_deep_learning/lab02/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing path\n",
    "import os\n",
    "\n",
    "os.getcwd()   # Verify where it is right now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190406bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution, go to lab directory:\n",
    "os.chdir('/home/jobquiroz/full_stack_deep_learning/lab02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2c3d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_recognizer',\n",
       " 'notebooks',\n",
       " 'lightning_logs',\n",
       " 'training',\n",
       " '.ipynb_checkpoints',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4e922",
   "metadata": {},
   "source": [
    "### CNN on synthethic handwritting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9309c112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\" width=\"15\" height=\"15\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import IPython.display as display\n",
    "\n",
    "randsize = 10 ** (random.random() * 2 + 1)\n",
    "\n",
    "Url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\"\n",
    "\n",
    "# run multiple times to display the same image at different sizes\n",
    "#  the content of the image remains unambiguous\n",
    "display.Image(url=Url, width=randsize, height=randsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f071f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic:\n",
      "tensor([[ 0.6151],\n",
      "        [-1.2588],\n",
      "        [ 1.4434],\n",
      "        [ 0.1989],\n",
      "        [-1.3881],\n",
      "        [ 1.1812],\n",
      "        [ 1.2038],\n",
      "        [-0.8398]])\n",
      "local:\n",
      "tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0089],\n",
      "        [0.0369],\n",
      "        [0.0386],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "generic_linear_transform = torch.randn(8, 1)\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "local_linear_transform = torch.tensor([\n",
    "    [0, 0, 0] + [random.random(), random.random(), random.random()] + [0, 0]]).T\n",
    "print(\"local:\", local_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e2c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7]])\n",
      "translation invariant:\n",
      "tensor([[0, 7, 6, 5, 4, 3, 2, 1],\n",
      "        [1, 0, 7, 6, 5, 4, 3, 2],\n",
      "        [2, 1, 0, 7, 6, 5, 4, 3],\n",
      "        [3, 2, 1, 0, 7, 6, 5, 4],\n",
      "        [4, 3, 2, 1, 0, 7, 6, 5],\n",
      "        [5, 4, 3, 2, 1, 0, 7, 6],\n",
      "        [6, 5, 4, 3, 2, 1, 0, 7],\n",
      "        [7, 6, 5, 4, 3, 2, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "generic_linear_transform = torch.arange(8)[:, None]\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "equivariant_linear_transform = torch.stack([torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1)\n",
    "print(\"translation invariant:\", equivariant_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b333a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.4791, -0.4836, -0.0016]]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the equivalent of torch.nn.Linear, but for a 1-dimensional convolution\n",
    "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "conv_layer.weight  # aka kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cd4c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution vector:\n",
      "tensor([-0.4791, -0.4836, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       grad_fn=<CatBackward0>)\n",
      "convolution matrix:\n",
      "tensor([[-0.4791, -0.4836, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.4791, -0.4836, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.4791, -0.4836, -0.0016,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.4791, -0.4836, -0.0016,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4791, -0.4836, -0.0016,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4791, -0.4836, -0.0016],\n",
      "        [-0.0016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4791, -0.4836],\n",
      "        [-0.4836, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4791]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_as_vector = torch.hstack([conv_layer.weight[0][0], torch.zeros(5)])\n",
    "print(\"convolution vector:\", conv_kernel_as_vector, sep=\"\\n\")\n",
    "conv_layer_as_matrix = torch.stack([torch.roll(conv_kernel_as_vector, ii) for ii in range(8)], dim=0)\n",
    "print(\"convolution matrix:\", conv_layer_as_matrix, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4da01d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample image\n",
    "display.Image(url=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48315452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\" width=\"1024\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://distill.pub/2018/building-blocks/\n",
    "display.Image(url=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\", width=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "933a9816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1024\"\n",
       "            height=\"720\"\n",
       "            src=\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d1_52.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        style=\"background: #FFF\";\n",
       "></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa004f6c710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\"conv2d0\", \"conv2d1\", \"conv2d2\", \"mixed3a\", \"mixed3b\"]\n",
    "layer = layers[1]\n",
    "idx = 52\n",
    "\n",
    "weight_explorer = display.IFrame(\n",
    "    src=f\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/{layer}_{idx}.html\", width=1024, height=720)\n",
    "weight_explorer.iframe = 'style=\"background: #FFF\";\\n><'.join(weight_explorer.iframe.split(\"><\"))  # inject background color\n",
    "weight_explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3efb8",
   "metadata": {},
   "source": [
    "## Applying convolutions to handwritten characeters: CNN's on EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5efa88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "text_recognizer.models.CNN??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc176b",
   "metadata": {},
   "source": [
    "## EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e7c52f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST dataset of handwritten characters and digits.\n",
      "\n",
      "    \"The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19\n",
      "    and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\"\n",
      "    From https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
      "\n",
      "    The data split we will use is\n",
      "    EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "emnist = text_recognizer.data.EMNIST()  # configure\n",
    "print(emnist.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4093eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw dataset from https://s3-us-west-2.amazonaws.com/fsdl-public-assets/matlab.zip to /home/jobquiroz/full_stack_deep_learning/data/downloaded/emnist/matlab.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "709MB [00:21, 34.6MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHA-256...\n",
      "Unzipping EMNIST...\n",
      "Loading training data from .mat file\n",
      "Balancing classes to reduce amount of data\n",
      "Saving to HDF5 in a compressed format...\n",
      "Saving essential dataset parameters to text_recognizer/data...\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "emnist.prepare_data()  # download, save to disk\n",
    "emnist.setup()  # create torch.utils.data.Datasets, do train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a94c7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jobquiroz/full_stack_deep_learning/data\n",
      "downloaded  processed  raw\n",
      "metadata.toml  readme.md\n"
     ]
    }
   ],
   "source": [
    "!echo {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname() / \"raw\" / \"emnist\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7ada2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMNIST Dataset\n",
       "Num classes: 83\n",
       "Mapping: ['<B>', '<S>', '<E>', '<P>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '?']\n",
       "Dims: (1, 28, 28)\n",
       "Train/val/test sizes: 260212, 65054, 53988\n",
       "Batch x stats: (torch.Size([128, 1, 28, 28]), torch.float32, tensor(0.), tensor(0.1693), tensor(0.3258), tensor(1.))\n",
       "Batch y stats: (torch.Size([128]), torch.int64, tensor(4), tensor(65))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c57f9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(iter(emnist.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9819efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACK0lEQVR4nM2UvarqUBCFd7Ij8Q/EU6SxUrDUiNj5BKISQawU9AkE0crW0sLCB0ihhYgSrAQLRbCySmNlYh0EQUj8S2I8RTjeyzWK8aS4X7mHWSz2zBoA/itQFMUwDEXRxxL2mSKCIMlkMhwOLxaL+Xz+O3s/iiRJSpIkimKlUrFAEQDg9/s7nY6iKAzDBAKBd9twHPf5fIYlDMMqlYooioIgZLNZCOFbinobz/OGLgiCGA6Hl8ulWCziOG6oYDA7AACCIARBFAoFh8PxTymRSMTj8f1+Px6PL5fLWzbvdgaDwXa7bTQaf/slCEIQBEVR+v2+4TK9AsOwcrl8Pp8lSdKnYbPZXC5XsVhUFIXjOIqiXrUbvqqq2uv1crlcNBpNpVIoirIs63a7M5kMAGCxWKzXa3M279jtdpIkaZo+HA6qqmqadrvdVFUVRXEymbw790f0idVqtdlspuuORqNqtRoKhT5UvOPxeFqtlizLkiRFIhEM+zDcf4AQ6j+w3W5LpRKCIL9VBD9rdL1el8uliVC+AMdxfY14ns/n888iZAIIYTabFQTBXMxfEwgEGIZRFIWmaYIgLFAEAFSrVVEUZVmOxWLWzMfpdLIsezweu92uBV8JAIAQUhR1Op04jovFYhYofn191ev1zWYjy3Kz2Xy8fp8QDAZXq5WmaRzHpdNp0yfu2ZHWg8gwzHQ61TTNrKhBine7Xbvd9nq9nU7ncDiYVXwKhNCCq2Et3xAg8Uwo/l2JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9FD0BA0D50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "idx = random.randint(0, len(xs) - 1)\n",
    "\n",
    "print(emnist.mapping[ys[idx]])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da420de",
   "metadata": {},
   "source": [
    "Now we have a data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "825cb735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = emnist.config()\n",
    "\n",
    "cnn = text_recognizer.models.CNN(data_config)\n",
    "cnn  # reveals the nn.Modules attached to our nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cc06081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: X\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADwElEQVR4nOWVS0/qWhTH202RYtny0gE1CBVjjBZInBCiAwYOnBFGRv0cjnXuyM/gzAF+ABwRlWAIPkATxJgWDRRIsAIV2tL2Dva9yOFwcm7uHZ41aPrY+9e1/uuxMeyPNvzfLwUA4DhuGIau6/8XShAEhmFms9lms1EUJUnSx8fHYDD4L1CSJCGEXq83FotRFGW32/1+/8LCQqVSyWQyqVRKEIRms2kYxgQ/JvwHx6emppaXl5eWlqLRaCwWs9lsFouFoiiKonw+n9/vpyjq/v4+lUrJsqxp2m+CxXE8EAjs7e3d3t4KgqAoij7Jut3uy8vL4eFhPB4nSRLHfy2jxWJhGOb09LRcLmuahvYb/9hE9PPz8/7+fiAQGOX+EL7dbg+FQpFIhKZptAhdR4VDqQcAYBhmtVrn5+e3t7cNw3h/f5dledxNAEAikTg7O1NVddRHdN/r9RqNRi6XSyaTyWSy0WjIsow+dTqdm5ubxcXFCZ4CABiGYRjGZDKhN0MHDcOoVqvFYjGdTvM8DwCAEIbD4dnZWSSay+VyOBwToGazmaZpt9s9BCmKUqvVZFnudDonJyfZbJbjOF3XAQC6ru/u7sbjcQAAQRAejycWiz08PKD6/YZCCFGtoEdEPD8/F0Wx2+1eX1/XajW0xzAMjuN4nh+GQhDE9PT0uKcAgI2Njc3NTZfLhd4g4tHRUa/XwzBstIVIkmRZdphxXddFUXx6ehq2799QHMd9Pp/NZhsKOhgMJEmSZXmsI3Ecp2l6a2uLZVlUAyhXSJkfoIZhVCoVSZJIkjSZTIZhOByOcDgcCoV4nu92u6qq4jjudDo9Hs/BwUEikSBJEu1ttVr5fP7t7W08fF3XOY6TJMnpdKL/QwiDweDOzk4mk+F5vt1uAwCCwSDLstFodLSLBEF4fHzsdDrYz+b3+3O5nCRJoz2jKEq9Xn99fS2VSqVSqV6vf319jS04Pj5eX18fRX1nX5IkjuO8Xq/VakVzE6XV7XY7nc5hPlEcQ9FkWRYEQRTFyVBRFAuFAsuyc3Nzw1rBcXwMNJRLUZRSqXR3d5dMJqvV6mSopmn5fJ6maYIgIITD2kLNKoqiJElI65mZmXQ6nc1mz87OJk7Vb6iu64VCAcOwdrvNMEwkEkEO9vt9VVWLxSLHcWazeW1tbWVl5fLy8uLiolwuK4ry85wen4MAAIvFAiEMhUII2mw20RHS7XYhhKurq3a7/erq6vPzU1VVbJJNHq4mkwlCiO77/b6maWi8ms1mCCEAoNVq/fb4+2PtL2p+YT06jr68AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9FE5106AD0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = cnn(xs[idx:idx+1])\n",
    "\n",
    "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf64bf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(torch.Tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fdad202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 64, 36864, 64, 1605632, 128, 10624, 83]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.numel() for p in cnn.parameters()]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2e222b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12544]), 12544)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_layer = [p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())][0]\n",
    "biggest_layer.shape, cnn.fc_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db6f3941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1605632"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the Linear layers, number of multiplications per input == nparams\n",
    "cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c458f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Conv2D layers, it's more complicated\n",
    "\n",
    "def approx_conv_multiplications(kernel_shape, input_size=(64, 28, 28)):  # this is a rough and dirty approximation\n",
    "    num_kernel_elements = 1\n",
    "    for dimension in kernel_shape[-3:]:\n",
    "        num_kernel_elements *= dimension\n",
    "    num_input_channels, num_kernels = input_size[0], kernel_shape[0]\n",
    "    num_spatial_applications = ((input_size[1] - kernel_shape[-2] + 1) * (input_size[2] - kernel_shape[-1] + 1))\n",
    "    mutliplications_per_kernel = num_spatial_applications * num_kernel_elements * num_input_channels\n",
    "    return mutliplications_per_kernel * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c6f45e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594884096"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df0ad36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of multiplications in the convolution to multiplications in the fully-connected layer is huge!\n",
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a37c6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_experiment.py [--logger [LOGGER]]\n",
      "                         [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                         [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
      "                         [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                         [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                         [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                         [--process_position PROCESS_POSITION]\n",
      "                         [--num_nodes NUM_NODES]\n",
      "                         [--num_processes NUM_PROCESSES] [--devices DEVICES]\n",
      "                         [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                         [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                         [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                         [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
      "                         [--overfit_batches OVERFIT_BATCHES]\n",
      "                         [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                         [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                         [--max_time MAX_TIME]\n",
      "                         [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                         [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                         [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                         [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                         [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                         [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                         [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                         [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
      "                         [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                         [--precision PRECISION]\n",
      "                         [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
      "                         [--weights_summary WEIGHTS_SUMMARY]\n",
      "                         [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                         [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                         [--deterministic [DETERMINISTIC]]\n",
      "                         [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                         [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                         [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                         [--detect_anomaly [DETECT_ANOMALY]]\n",
      "                         [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                         [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                         [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                         [--amp_level AMP_LEVEL]\n",
      "                         [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                         [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                         [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "                         [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
      "                         [--data_class DATA_CLASS] [--model_class MODEL_CLASS]\n",
      "                         [--load_checkpoint LOAD_CHECKPOINT]\n",
      "                         [--stop_early STOP_EARLY] [--batch_size BATCH_SIZE]\n",
      "                         [--num_workers NUM_WORKERS] [--fc1 FC1] [--fc2 FC2]\n",
      "                         [--fc_dropout FC_DROPOUT] [--optimizer OPTIMIZER]\n",
      "                         [--lr LR] [--one_cycle_max_lr ONE_CYCLE_MAX_LR]\n",
      "                         [--one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS]\n",
      "                         [--loss LOSS] [--help]\n",
      "\n",
      "optional arguments:\n",
      "  --data_class DATA_CLASS\n",
      "                        String identifier for the data class, relative to\n",
      "                        text_recognizer.data.\n",
      "  --model_class MODEL_CLASS\n",
      "                        String identifier for the model class, relative to\n",
      "                        text_recognizer.models.\n",
      "  --load_checkpoint LOAD_CHECKPOINT\n",
      "                        If passed, loads a model from the provided path.\n",
      "  --stop_early STOP_EARLY\n",
      "                        If non-zero, applies early stopping, with the provided\n",
      "                        value as the 'patience' argument. Default is 0.\n",
      "  --help, -h\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "                        If multiple loggers are provided and the `save_dir`\n",
      "                        property of that logger is not set, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
      "                        any of the individual loggers. Default: ``True``.\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
      "                        If ``True``, enable checkpointing. Default: ``None``.\n",
      "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\n",
      "                        deprecated in v1.5 and will be removed in v1.7. Please\n",
      "                        consider using ``enable_checkpointing`` instead.\n",
      "  --enable_checkpointing [ENABLE_CHECKPOINTING]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`. Default:\n",
      "                        ``True``.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        The value at which to clip gradients. Passing\n",
      "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
      "                        If using Automatic Mixed Precision (AMP), the\n",
      "                        gradients will be unscaled before. Default: ``None``.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        The gradient clipping algorithm to use. Pass\n",
      "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
      "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
      "                        norm. By default it will be set to ``\"norm\"``.\n",
      "  --process_position PROCESS_POSITION\n",
      "                        Orders the progress bar when running multiple models\n",
      "                        on same machine. .. deprecated:: v1.5\n",
      "                        ``process_position`` has been deprecated in v1.5 and\n",
      "                        will be removed in v1.7. Please pass :class:`~pytorch_\n",
      "                        lightning.callbacks.progress.TQDMProgressBar` with\n",
      "                        ``process_position`` directly to the Trainer's\n",
      "                        ``callbacks`` argument instead.\n",
      "  --num_nodes NUM_NODES\n",
      "                        Number of GPU nodes for distributed training. Default:\n",
      "                        ``1``.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        Number of processes for distributed training with\n",
      "                        ``accelerator=\"cpu\"``. Default: ``1``.\n",
      "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
      "                        `num_processes` or `ipus`, based on the accelerator\n",
      "                        type.\n",
      "  --gpus GPUS           Number of GPUs to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node Default:\n",
      "                        ``None``.\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and ``gpus`` or ``devices`` is an integer,\n",
      "                        pick available gpus automatically. This is especially\n",
      "                        useful when GPUs are configured to be in \"exclusive\n",
      "                        mode\", such that only one process at a time can access\n",
      "                        them. Default: ``False``.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on (1) Default: ``None``.\n",
      "  --ipus IPUS           How many IPUs to train on. Default: ``None``.\n",
      "  --log_gpu_memory LOG_GPU_MEMORY\n",
      "                        None, 'min_max', 'all'. Might slow performance. ..\n",
      "                        deprecated:: v1.5 Deprecated in v1.5.0 and will be\n",
      "                        removed in v1.7.0 Please use the\n",
      "                        ``DeviceStatsMonitor`` callback directly instead.\n",
      "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
      "                        How often to refresh progress bar (in steps). Value\n",
      "                        ``0`` disables progress bar. Ignored when a custom\n",
      "                        progress bar is passed to\n",
      "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
      "                        suitable value will be chosen based on the environment\n",
      "                        (terminal, Google COLAB, etc.). .. deprecated:: v1.5\n",
      "                        ``progress_bar_refresh_rate`` has been deprecated in\n",
      "                        v1.5 and will be removed in v1.7. Please pass :class:`\n",
      "                        ~pytorch_lightning.callbacks.progress.TQDMProgressBar`\n",
      "                        with ``refresh_rate`` directly to the Trainer's\n",
      "                        ``callbacks`` argument instead. To disable the\n",
      "                        progress bar, pass ``enable_progress_bar = False`` to\n",
      "                        the Trainer.\n",
      "  --enable_progress_bar [ENABLE_PROGRESS_BAR]\n",
      "                        Whether to enable to progress bar by default. Default:\n",
      "                        ``False``.\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training data (float) or a set\n",
      "                        number of batches (int). Default: ``0.0``.\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm. If using Automatic Mixed\n",
      "                        Precision (AMP), the gradients will be unscaled before\n",
      "                        logging them. Default: ``-1``.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Check val every n train epochs. Default: ``1``.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test). Default: ``False``.\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict. Default: ``None``.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to ``max_epochs\n",
      "                        = 1000``. To enable infinite training, set\n",
      "                        ``max_epochs = -1``.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None).\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
      "                        None``, will default to ``max_epochs = 1000``. To\n",
      "                        enable infinite training, set ``max_epochs`` to\n",
      "                        ``-1``.\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (``None``).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (``None``). The time duration can\n",
      "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches). Default: ``1.0``.\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Pass a\n",
      "                        ``float`` in the range [0.0, 1.0] to check after a\n",
      "                        fraction of the training epoch. Pass an ``int`` to\n",
      "                        check after a fixed number of training batches.\n",
      "                        Default: ``1.0``.\n",
      "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
      "                        How often to flush logs to disk (defaults to every 100\n",
      "                        steps). .. deprecated:: v1.5\n",
      "                        ``flush_logs_every_n_steps`` has been deprecated in\n",
      "                        v1.5 and will be removed in v1.7. Please configure\n",
      "                        flushing directly in the logger instead.\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps. Default: ``50``.\n",
      "  --accelerator ACCELERATOR\n",
      "                        Supports passing different accelerator types (\"cpu\",\n",
      "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\") as well as custom\n",
      "                        accelerator instances. .. deprecated:: v1.5 Passing\n",
      "                        training strategies (e.g., 'ddp') to ``accelerator``\n",
      "                        has been deprecated in v1.5.0 and will be removed in\n",
      "                        v1.7.0. Please use the ``strategy`` argument instead.\n",
      "  --strategy STRATEGY   Supports different training strategies with aliases as\n",
      "                        well custom strategies. Default: ``None``.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world. Default: ``False``.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32), half\n",
      "                        precision (16) or bfloat16 precision (bf16). Can be\n",
      "                        used on CPU, GPU, TPUs, HPUs or IPUs. Default: ``32``.\n",
      "  --enable_model_summary [ENABLE_MODEL_SUMMARY]\n",
      "                        Whether to enable model summarization by default.\n",
      "                        Default: ``True``.\n",
      "  --weights_summary WEIGHTS_SUMMARY\n",
      "                        Prints a summary of the weights when training begins.\n",
      "                        .. deprecated:: v1.5 ``weights_summary`` has been\n",
      "                        deprecated in v1.5 and will be removed in v1.7. To\n",
      "                        disable the summary, pass ``enable_model_summary =\n",
      "                        False`` to the Trainer. To customize the summary, pass\n",
      "                        :class:`~pytorch_lightning.callbacks.model_summary.Mod\n",
      "                        elSummary` directly to the Trainer's ``callbacks``\n",
      "                        argument.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`. .. deprecated:: v1.6\n",
      "                        ``weights_save_path`` has been deprecated in v1.6 and\n",
      "                        will be removed in v1.8. Please pass ``dirpath``\n",
      "                        directly to the :class:`~pytorch_lightning.callbacks.m\n",
      "                        odel_checkpoint.ModelCheckpoint` callback.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders. Default: ``2``.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        an exception is raised. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch. .. deprecated:: v1.5\n",
      "                        ``resume_from_checkpoint`` is deprecated in v1.5 and\n",
      "                        will be removed in v2.0. Please pass the path to\n",
      "                        ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks. Default: ``None``.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        Sets ``torch.backends.cudnn.benchmark``. Defaults to\n",
      "                        ``True`` if :paramref:`~pytorch_lightning.trainer.trai\n",
      "                        ner.Trainer.deterministic` is ``False``. Overwrite to\n",
      "                        manually set a different value. Default: ``None``.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If ``True``, sets whether PyTorch operations must use\n",
      "                        deterministic algorithms. Default: ``False``.\n",
      "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a non-negative integer to reload dataloaders\n",
      "                        every n epochs. Default: ``0``.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name. Default: ``False``.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --detect_anomaly [DETECT_ANOMALY]\n",
      "                        Enable anomaly detection for the autograd engine.\n",
      "                        Default: ``False``.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "                        Default: ``False``.\n",
      "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
      "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
      "                        data .. deprecated:: v1.5 Deprecated in v1.5.0 and\n",
      "                        will be removed in v1.7.0 Please set\n",
      "                        ``prepare_data_per_node`` in ``LightningDataModule``\n",
      "                        and/or ``LightningModule`` directly instead.\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins. Default:\n",
      "                        ``None``.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\"). Default: ``'native''``.\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...). By\n",
      "                        default it will be set to \"O2\" if ``amp_backend`` is\n",
      "                        set to \"apex\".\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention. Default:\n",
      "                        ``False``.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets. Default:\n",
      "                        ``\"max_size_cycle\"``.\n",
      "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
      "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
      "                        stochastic-weight-averaging/>`_. Default: ``False``.\n",
      "                        .. deprecated:: v1.5 ``stochastic_weight_avg`` has\n",
      "                        been deprecated in v1.5 and will be removed in v1.7.\n",
      "                        Please pass :class:`~pytorch_lightning.callbacks.stoch\n",
      "                        astic_weight_avg.StochasticWeightAveraging` directly\n",
      "                        to the Trainer's ``callbacks`` argument instead.\n",
      "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
      "                        If set to True, will terminate training (by raising a\n",
      "                        `ValueError`) at the end of each training batch, if\n",
      "                        any of the parameters or the loss are NaN or +/-inf.\n",
      "                        .. deprecated:: v1.5 Trainer argument\n",
      "                        ``terminate_on_nan`` was deprecated in v1.5 and will\n",
      "                        be removed in 1.7. Please use ``detect_anomaly``\n",
      "                        instead.\n",
      "\n",
      "Data Args:\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of examples to operate on per forward step.\n",
      "                        Default is 128.\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of additional processes to load data. Default\n",
      "                        is 4.\n",
      "\n",
      "Model Args:\n",
      "  --fc1 FC1\n",
      "  --fc2 FC2\n",
      "  --fc_dropout FC_DROPOUT\n",
      "\n",
      "LitModel Args:\n",
      "  --optimizer OPTIMIZER\n",
      "                        optimizer class from torch.optim\n",
      "  --lr LR\n",
      "  --one_cycle_max_lr ONE_CYCLE_MAX_LR\n",
      "  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS\n",
      "  --loss LOSS           loss function from torch.nn.functional\n"
     ]
    }
   ],
   "source": [
    "%run training/run_experiment.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8cfdafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        .. deprecated:: v1.5 Trainer argument\r\n",
      "                        ``terminate_on_nan`` was deprecated in v1.5 and will\r\n",
      "                        be removed in 1.7. Please use ``detect_anomaly``\r\n",
      "                        instead.\r\n",
      "\r\n",
      "Data Args:\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "                        Number of examples to operate on per forward step.\r\n",
      "                        Default is 128.\r\n",
      "  --num_workers NUM_WORKERS\r\n",
      "                        Number of additional processes to load data. Default\r\n",
      "                        is 4.\r\n",
      "\r\n",
      "Model Args:\r\n",
      "  --conv_dim CONV_DIM\r\n",
      "  --fc_dim FC_DIM\r\n",
      "  --fc_dropout FC_DROPOUT\r\n",
      "\r\n",
      "LitModel Args:\r\n",
      "  --optimizer OPTIMIZER\r\n",
      "                        optimizer class from torch.optim\r\n",
      "  --lr LR\r\n",
      "  --one_cycle_max_lr ONE_CYCLE_MAX_LR\r\n",
      "  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS\r\n",
      "  --loss LOSS           loss function from torch.nn.functional\r\n"
     ]
    }
   ],
   "source": [
    "!python training/run_experiment.py --help --model_class CNN --data_class EMNIST  | tail -n 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7fe0ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: training/logs/lightning_logs\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | model          | CNN       | 1.7 M \n",
      "1 | model.conv1    | ConvBlock | 640   \n",
      "2 | model.conv2    | ConvBlock | 36.9 K\n",
      "3 | model.dropout  | Dropout   | 0     \n",
      "4 | model.max_pool | MaxPool2d | 0     \n",
      "5 | model.fc1      | Linear    | 1.6 M \n",
      "6 | model.fc2      | Linear    | 10.7 K\n",
      "7 | train_acc      | Accuracy  | 0     \n",
      "8 | val_acc        | Accuracy  | 0     \n",
      "9 | test_acc       | Accuracy  | 0     \n",
      "---------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c12eb37f6104449aa1324c0924f57e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4cc91288b6431685943fbc56107842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/jobquiroz/full_stack_deep_learning/lab02/training/logs/lightning_logs/version_0/epoch=0000-validation.loss=0.574.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test/acc            0.7847299575805664\n",
      "        test/loss           0.5782811045646667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = int(torch.cuda.is_available())  # use GPUs if they're available\n",
    "\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8102ac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/logs/lightning_logs/version_0/epoch=0000-validation.loss=0.574.ckpt'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use a sequence of bash commands to get the latest checkpoint's filename\n",
    "#  by hand, you can just copy and paste it\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs\"  # find avoids issues with \\n in filenames\n",
    "filter_to_ckpts = \"grep \\.ckpt$\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1447ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/logs/lightning_logs/\r\n",
      "training/logs/lightning_logs/version_0\r\n",
      "training/logs/lightning_logs/version_0/hparams.yaml\r\n",
      "training/logs/lightning_logs/version_0/epoch=0000-validation.loss=0.574.ckpt\r\n",
      "training/logs/lightning_logs/version_0/events.out.tfevents.1661450371.instance-1.29046.0\r\n",
      "training/logs/lightning_logs/version_0/events.out.tfevents.1661450411.instance-1.29046.1\r\n"
     ]
    }
   ],
   "source": [
    "!find training/logs/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b4579cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training.util\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"CNN\",\n",
    "    \"data_class\": \"EMNIST\"})\n",
    "\n",
    "\n",
    "_, cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "reloaded_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a8a9c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: E\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACv0lEQVR4nO2UvUvrUBjGk5wj0bSWE6yiiHiodBJB6KSLBQUROoi6iEN3B8HBTejqpH+BOrgIHSRLly6Ng24OtlJQKC0VaavSkkZKej7icK7eq/Zq672jvyEJSc7D835K0g//G7mdnxRFURRFPHPOOeffFFVVdWhoqFKpDA4OTk5OTkxMCMVMJpNIJBzHadv3C36//+DgwLbtXC739PREX+CcE0I2NzchhJ8cb/1N1/WpqSlN0zDGzWbz7u6uUCgghMbHx1/z0DErKyvlctl1Xc759fV1Npt9eHiglJZKpcPDw+7u7o4VAQDxeJwQImrCGBNX27aj0ejAwMCXCi0KhTG+urrq6ekxTdPn83m9Xsuyjo+Pk8lkOp12XfdL0RY57erqghAyxnZ2dm5ubgAAhJDb21tKaTuBtkZVVcMwKKUnJyfBYBAh9Hmt23LabDYvLy8XFhbm5+cBAI+Pj5lM5ujo6P7+vp3Y3wMA6O/vj0ajlUqFv+X8/Bxj3LFTAEAkEllbWwuHw319fX9mkBCSTqdt2+7MI4RwY2Oj0WgwxkqlUiwWC4VCwRcwxgCAzhQlSQoEAoZhcM5zudzy8vJ32vsdsizv7+8TQgghu7u7stzW6hJACAOBwNLSUiwWW19fF0MMJUlyXTefzzuOo2na7Ozs2NhYPp//2JUQQlmWEUIIIfFGUZRIJLK6ujoyMkIpNQzjl0tx0zRta2tre3sbANBoNLLZbCqVqtfrr4q9vb0zMzM+nw8hpOu6sMIYsyyrWCyapplIJE5PT4WV35FijPf29ubm5jwejyRJH52KWjHGqtVqrVazLMs0zbOzs4uLi3K57DjO6/J+kz5VVYeHhxcXF6enp0dHRxVF8Xq9nPNCoSBaql6vp1KpYrFYq9Vc1/3b4LaoCYRQ13WPxyPLshhQ27ar1aqQ+KcN8MO3eQbOp3d5DtqTNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F9FE406A710>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = reloaded_model(xs[idx:idx+1])\n",
    "\n",
    "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59539dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | model          | CNN       | 1.7 M \n",
      "1 | model.conv1    | ConvBlock | 640   \n",
      "2 | model.conv2    | ConvBlock | 36.9 K\n",
      "3 | model.dropout  | Dropout   | 0     \n",
      "4 | model.max_pool | MaxPool2d | 0     \n",
      "5 | model.fc1      | Linear    | 1.6 M \n",
      "6 | model.fc2      | Linear    | 10.7 K\n",
      "7 | train_acc      | Accuracy  | 0     \n",
      "8 | val_acc        | Accuracy  | 0     \n",
      "9 | test_acc       | Accuracy  | 0     \n",
      "---------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a663bf522014526aeb015eb5f331da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506f950d61a14a38b7ebe9d3320c9faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/jobquiroz/full_stack_deep_learning/lab02/training/logs/lightning_logs/version_1/epoch=0000-validation.loss=0.536.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test/acc            0.7911202311515808\n",
      "        test/loss           0.5394253730773926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "\n",
    "\n",
    "# and we can change the training hyperparameters, like batch size\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus} \\\n",
    "  --batch_size 64 --load_checkpoint {latest_ckpt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f5588",
   "metadata": {},
   "source": [
    "## EMNISTLines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f970979",
   "metadata": {},
   "source": [
    "Let's kick the realism up one notch by building lines of text out of our characters: synthesizing data for our model.\n",
    "\n",
    "To build fake handwriting, we'll combine two things: real handwritten letters and real text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a177155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jobquiroz/full_stack_de\n",
      "[nltk_data]     ep_learning/data/downloaded/nltk...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generate text sentences using the Brown corpus.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_recognizer.data.sentence_generator import SentenceGenerator\n",
    "\n",
    "sentence_generator = SentenceGenerator()\n",
    "\n",
    "SentenceGenerator.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f719a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new members\n",
      "sport There\n",
      "were clean well\n",
      "made me\n",
      "McFeeley and\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence_generator.generate(max_length=16) for _ in range(5)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "84b66dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMNIST Lines dataset: synthetic handwriting lines dataset made from EMNIST characters.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_lines = text_recognizer.data.EMNISTLines()  # configure\n",
    "emnist_lines.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1149747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset generating data for train...\n",
      "EMNISTLinesDataset generating data for val...\n",
      "EMNISTLinesDataset generating data for test...\n",
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EMNIST Lines Dataset\n",
       "Min overlap: 0\n",
       "Max overlap: 0.33\n",
       "Num classes: 83\n",
       "Dims: (1, 28, 896)\n",
       "Output dims: (32, 1)\n",
       "Train/val/test sizes: 10000, 2000, 2000\n",
       "Batch x stats: (torch.Size([128, 1, 28, 896]), torch.float32, 0.0, 0.07364349067211151, 0.23208004236221313, 1.0)\n",
       "Batch y stats: (torch.Size([128, 32]), torch.int64, 3, 66)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_lines.prepare_data()  # download, save to disk\n",
    "emnist_lines.setup()  # create torch.utils.data.Datasets, do train/val split\n",
    "emnist_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33e7c5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 896]), torch.Size([128, 32]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_xs, line_ys = next(iter(emnist_lines.val_dataloader()))\n",
    "line_xs.shape, line_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41533511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-o-e-s-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F9FE4042890>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_line_labels(labels):\n",
    "    return [emnist_lines.mapping[label] for label in labels]\n",
    "\n",
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "print(\"-\".join(read_line_labels(line_ys[idx])))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fee2b1",
   "metadata": {},
   "source": [
    "## LineCNNSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41654a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LineCNNSimple(\n",
       "  (cnn): CNN(\n",
       "    (conv1): ConvBlock(\n",
       "      (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=83, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
    "line_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37a87822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-H-H-H-H-h-H-v-H-H-H-H-H-v-v-H-H-v-H-H-H-H-B-H-v-v-v-H-v-H-H-H\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F9FB065F7D0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = line_cnn(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf6a3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | LineCNNSimple | 1.7 M \n",
      "1 | model.cnn | CNN           | 1.7 M \n",
      "2 | train_acc | Accuracy      | 0     \n",
      "3 | val_acc   | Accuracy      | 0     \n",
      "4 | test_acc  | Accuracy      | 0     \n",
      "--------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2436de99d70f40dd9475c7c10ab215be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2261ebe2b3433395f2ce7746406ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/jobquiroz/full_stack_deep_learning/lab02/training/logs/lightning_logs/version_2/epoch=0001-validation.loss=1.380.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test/acc            0.6737968921661377\n",
      "        test/loss           1.4335458278656006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run training/run_experiment.py --model_class LineCNNSimple --data_class EMNISTLines \\\n",
    "  --batch_size 32 --gpus {gpus} --max_epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "991fbf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/logs/lightning_logs/version_2/epoch=0001-validation.loss=1.380.ckpt\n"
     ]
    }
   ],
   "source": [
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"LineCNNSimple\",\n",
    "    \"data_class\": \"EMNISTLines\"})\n",
    "\n",
    "\n",
    "_, line_cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "print(latest_ckpt)\n",
    "\n",
    "reloaded_lines_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=line_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "64ca3800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a- -g-e-n-e-n-l-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAAcCAIAAAC/GkalAAAR1klEQVR4nO3da0yT1x8H8OdW+pQCLYVSEKEFpOIAIfpCUfASIUgqN+emLjpfuGRjqHNqxpSpLDoTIUazLVmyF9OIy9C4AjOI1wkFozFOkQJekNIKCCLXlkqfts/z/F+c/fvnj4ooN91+n1dQ6OFHz5tvznnO72AYAAAAAAAAkwif6gLGDUVRGIZxHMdx3FTXAl6MIAgMw2CCAAAAgH+5dz6A+vr6BgcHr127dunSpWKxWK/X5+TkGI1GnudHPwhJkjKZTCqVom8dDseTJ08YhhlLVMJx/LVqmOh6phxN00lJSRiGXbhwgWGYqS4HAAAAAFOGmuoC3hxN09OmTdu9e3dCQkJQUBCO4xiGzZgxY2Bg4Kuvvurq6hr9OF5eXvHx8ZGRkQRBcBxntVorKipMJpPZbHY4HEN/k6bpvr6+5wehKArHcalUilIjQRDBwcHt7e2NjY2vG7ZGX887hKKorKysffv2NTQ06PV6o9E41RUBAAAAYMq8IoCSJMmy7OSUMnpCoTAiIuLYsWNqtZqiqJaWll27dul0uuzs7MzMzOjoaA8Pj1cGUKFQGBgYKJVKU1NTo6KiEhISvL290bKl0+lsa2s7duxYWVlZbW0tWshUqVTr169XKpU5OTnd3d2ucUiSVCqV6enpUqk0MjISpUYMw8RicWdn56FDh4qKikbzGb5uPRNqIubdy8vLzc2tsrLyyZMn4zsyAAAAAP4h3N3dQ0NDN27cqFKphEIhQRAkSU51URiGYUKhcMOGDbdv33Y6nQMDAwUFBampqRRFKRSKx48fsyy7d+9emqZHHoQkydDQ0O3btxcUFNTU1HR0dDgcDm4Im82m1Wo3bNggFAoxDKMoavv27WazeXBwcM6cOUPHSU9PLy4utlgsdrvd6XQOHYRl2aampqVLl6JIOo71TJwJmneBQPDtt9/abLaVK1e+8tMAAAAAwL+RUCiMiYnZsGHDhQsXUlNTFQqFXC5XKpUCgWBqCwsLCyssLHQ4HIODg0ePHo2JiUE7776+vkePHuU4jmEYtVo9wgg4joeEhAxNjSzLoocs79+/f//+fYPB0NXVxXGcxWK5ceNGaGgoRVFyufz06dMoX27btg0deHJ3d09PT3/48OGwsDhMQUGBWCwe33peNtTYPt0JnHeVSnXz5s2BgQGVSjXGoQAAAADwrhseZQQCgUQi0Wg0W7duDQgI8Pb2bm1tVavV8+bN8/T0zMrKam1tdTqdU1IrTdOnT5+ePXu2wWDIy8tzbW3jOL5ixQqNRsOy7NmzZ00m08tG8PX1DQwM3LFjx7x580JCQtBGc3Nzs16v//XXX/V6PcdxKpUqNTU1KytLJBIplcrY2FiLxYK219HSXWBg4LRp05YuXbp169aIiAjXkqTT6aypqSEIoqKiwmq1xsbGJiUl0TSdnZ3t4+PzwsdS37iep0+fDh1HKBQmJycvW7bsyJEjzc3NL/zfh8XWYZM40fPu4+Mjk8k6Ozth/x0AAAAAwwOoRCKJiYnJyMgIDw8XCoU4js+ePTskJGTWrFnd3d1ubm5jX2Z7Y2q1eubMmXa7PS8vT6vVuh5S9PPzy83N9fHxMRgMx44de9mhH5IkY2Ji5s+fv2DBgoCAAJIkGYbp7++vqqqqqKjQ6XS9vb08z1MUhb4gSdLDw0OlUt26dauvr6++vj48PJwgiOTkZC8vrxUrVvj5+aFPg2XZ7u5unU73yy+/PHjwoL293el0KpXK3NxcjUbj6+sbFxfn7e09LICOpZ5hATQwMPCbb76JiIgwGo2HDx8e+iOKoqZPny6TyTQajWvv22KxFBcXt7a2uo40Tei8y+Xyr7/+Ojg4uKmp6R09RAUAAACAcfR/AZSmaY1Gk5GRodFoSJK02+1dXV2RkZECgYAgCBzHo6Kient7h6WfSSqUotLS0miaNhqNf/zxh91uRw8prlmzZuXKlWFhYc+ePVu1alVtbe0L3452ujdt2jR//nyFQoFhWFdXV1lZWUVFRXl5eX9/P8MwOI67ubkpFAqpVIrylkAg8PT0FAgEKICmpaURBKFWq4OCgjw9PdHInZ2dVVVVRUVFVVVVaK8cvd7U1LRz506z2bx58+bnn3ocYz1Dh3J3d//444+joqLMZjNa/UUPbnp7e2dlZaWmps6cOZMgiMbGxkuXLlksFhzHFy1atG3bti+++OL333/HJn7epVJpdHQ0PPoJAAAAAOR/AZQgiKSkpNzc3OnTp1MU1dzcfO3atZKSkvz8/KCgIAzDvL294+LijEbjlARQjuMaGhpYlg0ICNixY4fRaPz888+9vLyCgoJommYYpqSk5N69ey88IY7jOE3T8+bNmz17NmqT5HQ679y5U1JSotfru7u7OY4jCEIkEikUisTExKioKBT4eJ5HgdL1BY7jFEW50ufg4GBOTk55eXl3d/ewY+M8z3d1dfX09ExEPUMtW7Zs3bp1NE2fPXv26tWrbm5u8fHxixcvjoyMTE5OFggE58+fv3XrVnFxcWNjo8PhwHFcqVRWVVWtXr26uLgYw7BJmHccx6dw7RwAAAAAb5X/C6BqtTogIEAoFHIcV1dXd/nyZYPB4HQ6UaojCCIkJCQ0NPTOnTuT35uJ47iysrITJ06kpKTs3LkTvYjiIM/zdXV1e/bseX7zHS0ZhoSEREdH5+bmqlQqHMd7enr0ev2hQ4cqKytxHA8KCnJzc4uKioqLi1Or1cuWLUOnvwcHB9va2mpqanp7e4eO6eowPzg4qNVqT506NTg4OELZE13PnDlzUFK8efMmRVFbtmxZvnx5fHx8b2/v+fPn9Xp9YWFhS0vL0L1vk8nU39+P/jqO4xM9715eXgKBgGXZ2trad7qXPgAAAADGxd8BFF29ExcXJxKJcBznOO7Ro0etra0RERESiQTtwxIEER8fz/O8Tqfr7Oyc/FoZhsnJyTlz5kxcXJyvr6/RaJTL5Z999hmGYT/++GNbW9vzbwkMDFyxYsWWLVukUqmPj49rEQ79L3FxcWKxeMmSJR4eHt7e3t7e3gRB8DzvcDhMJpNOp7t06dLFixcZhhnaimho+szLyxs5fdbX1w+NXONVj2tAPz+/9evXo035rKysL7/8UiwWd3R0HDhw4MSJE0aj8YWBTyqV2u32gwcPYhg2CfO+ePFihULR09Nz8uRJCKAAAAAA+DuAymSyhQsXxsfHowf1SJJcvXr1kiVLBgcHfXx8XE/v+fj4JCQkpKSknDp1ymazTUJH9GE6Ozu1Wq1Wq8VxnCTJLVu2EARhNBpPnTr1wrNHFEVJJJLAwEB0sAYtXnp6ekZGRgYEBPA8LxAIFAoFRVEkSaIE1t3d3dHRcfHiRZ1OV1dXxzDM8/8mwzDFxcV5eXkGg2GEalGeQ5GLoiiCIMa3HhQNlUolGmf69OlGozE/P//KlSsNDQ02m+1lhbm7u1dVVVVVVU3OvEskEoqiHj16dOvWrdd6IwAAAAD+kf4OoGKxWKlUenh4uH4gk8k8PDzsdvvQsyMkSUokkqSkpOrq6sePH4+w+DfReJ7HcVwikfA839PT87Kz1c+ePWttbWUYBrWmR8kJx3EvLy8vL6+ho3V3d7e0tBgMhpMnTzY2NqIrNF+2XFdfX79nz56R0yeGYRRFpaSkUBSlUqlKS0u///770tLScaxHJBItXLiQJEme520228GDB48fP/6yNkxDmUymzZs3ox5PEz3vNE1nZGRgGFZWVtbS0jL6NwIAAADgn+rvAGq1Wk0m08DAgLu7O4ZhaOMVXX2OfsG16CUUCjMzMzEMu3jxYnl5OeoQhD3XV3ISzJo1Kz093el0njlzZpR/neM4u92O7hka9np9ff21a9eam5urq6vNZvMIy4dOp7OioqK9vX00fxHFOIFAoFKphu65j0s9GRkZKNuh5wEKCgqePXs2mqqw/z6cOgnz7u/vP2PGDLSaO/lL5gAAAAB4C/0dQHt6eq5evarT6TIyMtCxHvS6zWbr6OhwOp0ymUwmk6EXRSLR+++/n5iYmJaWVldXx3Gc2WwuLi5ub29/WQ/OiZCZmalWq1taWkbY2BWLxcHBwWKxGAWgysrK69eva7XagYGBYauJfX19vb296O6iV/5pi8Uymn6W6MgRNuSOovGqB8fxsLCwQ4cO+fn5NTc379mzR6vVvsGC9CTMu9VqtVqt6LA/AAAAAADmCqAsy5rN5pqamvj4eIlEIhKJ0IWQDx48uHTp0uDg4IIFCxYtWuQ6iyMUCn18fOLj46OiojAM6+vre/jw4cDAwKQFUIFAsHbtWpqmCwsLz507N5q3cBxXXV1dVFT0sm5N4y48PDwxMRHDMJ7nn++gNJZ65HJ5bm6uXC7HMEyr1ZaUlLzZ4xCTMO9isXiEm0gBAAAA8C/0vzZMNpvt559/vn//flRUlEajMRqN9fX1qHkkx3HLly8PCgoKCwtzLeaRJCmXy1EGcjgcsbGx9+7de/62yYkgEAiUSqVMJmNZVq/Xj9AbyOFwWK1Wp9M5xtvMXWuZaEyLxfLKt1AUlZSUFB4ejr5FrewZhhmXetCRIBzHu7q6ysvLx/Iw7kTPu0AgGPtV8gAAAAD4JyNJkqZplUoll8tdDwJiGEbT9EcffXT79u2Ojg6WZdHWsGthj2GY3bt3u8LWxNWWmpqan5/f0NDQ2dnpdDpLS0tdV7G/kEgkmjNnzsOHD+12u9PpvHz58v79+0NDQ8ViMTUE6jc0wjgqlermzZvoXzYYDHPnzn1ltXK5/PTp06ibJsdx9+7dU6vV41IPSZI3btxgWdZutxcUFLyyktGYuHkPDw+32+12u33v3r3D7qMHAAAAwL/T8EDAsizLsi0tLcO2jBmGQRfkxMbGJiUloTMrro1jp9NptVon9JpvsVicmJiYn5/P83xnZ6darWYYRqfTjbzp73A4+vr6enp6FAqFWCyOjo6WyWSNjY319fX9/f2u30HF22w2lLGe3y53PQrJ8zzDMGaz+ZUFS6XSyMhI1MgT9WDv7e0dl3pQ4/1Zs2Y1NjbqdLrX/SRfaKLnfZTLxgAAAAD4N3i92xHd3d39/f3XrVuXlpYmlUoDAgLQ7qrRaFyzZs3du3fHvTETjuPR0dGZmZnr16/39fU9fPhwcXGxVqsNCQk5fvx4Tk7OK1ujC4XCVatWLV++fNWqVUKhEHU4GhgYcMUmi8ViMpksFsvTp0+tVmtXV5der6+qqnLlMJIkN23atH//frFYzHHcxo0bi4qKRg6+BEFkZGT89NNPaKvaYDB8+OGH6LDU2OtBgwQEBHR0dIxwWn8cjWXe586de/369ZaWlg8++OCvv/6ahGoBAAAA8JZ7vS1RdBtkSUmJ1Wr19/dfsmQJ6l5pNBrR8t6416dUKqurqz08PJqamrKzs//888/s7OzQ0FCr1bpv377RXMyDmsbX1dUplcq5c+dSFCWTyYZ2ROJ5PjY2lmVZp9PpcDhu3LiBjge5RmBZ9syZM4sXL05JSeE47tq1a688a0UQxHvvvYeOfjMMU1VV9ejRo/GqBw1iNBpH+RmO3RvPO0VRGo2GJMmenp7u7u5JKxgAAAAAb7PXC6BoA7q2trahoQHd2YNWwhwOR1tb20S0ApXJZCKRiOO4AwcO6HS6Tz/99JNPPmEYprS09IV3b74QwzBtbW3nzp0TiUSenp7e3t4eHh5DT8bwPI/aHlkslitXrty9e3fYsXSTyfTDDz/cvn0bw7DRHLTiOO7OnTsmkykkJOTu3btHjhwZGr/GXs8kG8u84zjOsmxlZeWTJ08mq14AAAAAvNVebwt+khEEkZaW9ttvv+E4XlhYmJCQEBYWRhDEKDffh6Fp2t/f383NLSoqSqVSuU61Y/9t/F5XV4fawtvt9ue7bxIEgbrKjzJn0zSdnJyclpZWWlp6/vz55xdNx1jPO4GiqF27duXm5n733XcHDhyY/NsKAAAAAPAWeqsDKIZhvr6+WVlZBEGwLIv2c3meX7t2rcFgeINFQYqicByXSqVisXhYb6C+vr6+vj6e58cxJJEk6enpabFYXtYoapLrmRIqlSojI6OkpGQynxkAAAAAABg3qEvRVFcBXo+rjz0AAAAAAAAAAAAAAJPtP4tba7rB5tIaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F9FE405DE50>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = reloaded_lines_model(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ec3889be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5359)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_token = emnist_lines.emnist.inverse_mapping[\"<P>\"]\n",
    "torch.sum(line_ys == padding_token) / line_ys.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf082e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0feb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
